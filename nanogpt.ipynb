{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["nOTT9r3UqXHb","dA9I5yBdPGnT","paPGTnoZP0SV","CGQAegsZQH4e","9Il3MOkxh9Gu","tPb9rcNVRktD","zc7x_2ZOSwds","HCeAMYnKS7iF","7Wgr06ZLTNvP","evGWJ5IQTNfh"],"mount_file_id":"1-8d6hucTibNQrLvxmQFiaRZ8ek9N9k4n","authorship_tag":"ABX9TyOipOqKebTv/c76RSUnv1Vp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#1. Intro\n"],"metadata":{"id":"nOTT9r3UqXHb"}},{"cell_type":"code","source":["# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n","!wget https://gist.githubusercontent.com/flackend/18014f35d32b37c595b138f666b2108f/raw/99494b71652af807e77560b1d83ebbc5ed4c2f32/sorcerers-stone.txt"],"metadata":{"id":"KARDeenzNEsx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711666876311,"user_tz":-330,"elapsed":417,"user":{"displayName":"Lokesh Dandumahanti","userId":"04322942827821170944"}},"outputId":"e21554db-64a6-40e8-ed79-dcbaa1342a74"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-03-28 23:01:16--  https://gist.githubusercontent.com/flackend/18014f35d32b37c595b138f666b2108f/raw/99494b71652af807e77560b1d83ebbc5ed4c2f32/sorcerers-stone.txt\n","Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n","Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 459564 (449K) [text/plain]\n","Saving to: ‘sorcerers-stone.txt’\n","\n","\rsorcerers-stone.txt   0%[                    ]       0  --.-KB/s               \rsorcerers-stone.txt 100%[===================>] 448.79K  --.-KB/s    in 0.04s   \n","\n","2024-03-28 23:01:16 (11.9 MB/s) - ‘sorcerers-stone.txt’ saved [459564/459564]\n","\n"]}]},{"cell_type":"markdown","source":["#2. Reading and exploring the data"],"metadata":{"id":"dA9I5yBdPGnT"}},{"cell_type":"code","source":["# read it in to inspect it\n","with open('sorcerers-stone.txt', 'r', encoding='utf-8') as f:\n","    text = f.read()"],"metadata":{"id":"nbXThzYzqhS1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"length of dataset in characters: \", len(text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rW5Vh-_009x_","executionInfo":{"status":"ok","timestamp":1711667093020,"user_tz":-330,"elapsed":7,"user":{"displayName":"Lokesh Dandumahanti","userId":"04322942827821170944"}},"outputId":"b095f5af-b893-4f21-d989-29c45f375079"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["length of dataset in characters:  441832\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vZduYxLyqNQR","executionInfo":{"status":"ok","timestamp":1711667093020,"user_tz":-330,"elapsed":5,"user":{"displayName":"Lokesh Dandumahanti","userId":"04322942827821170944"}},"outputId":"b5690fe7-c814-4e68-a985-f957b9f3d3b7"},"outputs":[{"output_type":"stream","name":"stdout","text":["THE BOY WHO LIVED\n","\n","Mr. and Mrs. Dursley, of number four, Privet Drive,\n","were proud to say that they were perfectly normal,\n","thank you very much. They were the last people you’d\n","expect to be involved in anything strange or\n","mysterious, because they just didn’t hold with such\n","nonsense.\n","\n","Mr. Dursley was the director of a firm called\n","Grunnings, which made drills. He was a big, beefy\n","man with hardly any neck, although he did have a\n","very large mustache. Mrs. Dursley was thin and\n","blonde and had nearly twice the usual amount of\n","neck, which came in very useful as she spent so\n","much of her time craning over garden fences, spying\n","on the neighbors. The Dursley s had a small son\n","called Dudley and in their opinion there was no finer\n","boy anywhere.\n","\n","The Dursleys had everything they wanted, but they\n","also had a secret, and their greatest fear was that\n","somebody would discover it. They didn’t think they\n","could bear it if anyone found out about the Potters.\n","Mrs. Potter was Mrs. Dursley’s sister, but they hadn’t\n"]}],"source":["#let's look at the first 1000 characters\n","print(text[:1000])"]},{"cell_type":"markdown","source":["#3. Tokenization and train/val split\n"],"metadata":{"id":"paPGTnoZP0SV"}},{"cell_type":"markdown","source":["1. here basically all the characters are sorted and then assigned a number for each one of them.\n","2. then each of the character is then encodeded to a number till whole dataset is converted into a string of numbers\n","3. then it is broke into training and testing dataset\n","4. After that a neuron is"],"metadata":{"id":"fXTe4niX09fl"}},{"cell_type":"code","source":["#here are all the unique characters that occur in this text\n","\n","chars = sorted(list(set(text)))\n","vocab_size = len(chars)\n","print(''.join(chars))\n","print(vocab_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e7spV-TD0PFI","executionInfo":{"status":"ok","timestamp":1711667095518,"user_tz":-330,"elapsed":413,"user":{"displayName":"Lokesh Dandumahanti","userId":"04322942827821170944"}},"outputId":"19e8608a-e3dd-418f-8e14-ce38b7c61c94"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," !\"'(),-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWYZ\\abcdefghijklmnopqrstuvwxyz—‘’“”•■\n","83\n"]}]},{"cell_type":"code","source":["#create a mapping from characters to integers\n","\n","stoi = { ch:i for i,ch in enumerate(chars) }\n","itos = { i:ch for i,ch in enumerate(chars) }\n","encode = lambda s: [stoi[c] for c in s] # encoder : take a string, output a list of integers\n","decode = lambda l: ''.join([itos[i] for i in l])\n","\n","print(encode(\"hi there\"))\n","print(decode(encode(\"hi there\")))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R9p3K8SN-dKu","executionInfo":{"status":"ok","timestamp":1711667095518,"user_tz":-330,"elapsed":4,"user":{"displayName":"Lokesh Dandumahanti","userId":"04322942827821170944"}},"outputId":"cfc119dc-cda7-47f7-8f4e-a479673900b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[57, 58, 1, 69, 57, 54, 67, 54]\n","hi there\n"]}]},{"cell_type":"code","source":["# let's now encode the entire text dataset and store it into a'torch.Tensor\n","import torch # we use PyTorch: http://pytorch.org\n","data = torch.tensor(encode(text), dtype = torch.long)\n","print(data.shape, data.dtype)\n","print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wp9Ua1mU_V2E","executionInfo":{"status":"ok","timestamp":1711667099814,"user_tz":-330,"elapsed":4299,"user":{"displayName":"Lokesh Dandumahanti","userId":"04322942827821170944"}},"outputId":"3155602d-7c5d-4995-900e-186cb398b6ae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([441832]) torch.int64\n","tensor([43, 31, 28,  1, 25, 38, 47,  1, 46, 31, 38,  1, 35, 32, 45, 28, 27,  0,\n","         0, 36, 67,  9,  1, 50, 63, 53,  1, 36, 67, 68,  9,  1, 27, 70, 67, 68,\n","        61, 54, 74,  7,  1, 64, 55,  1, 63, 70, 62, 51, 54, 67,  1, 55, 64, 70,\n","        67,  7,  1, 39, 67, 58, 71, 54, 69,  1, 27, 67, 58, 71, 54,  7,  0, 72,\n","        54, 67, 54,  1, 65, 67, 64, 70, 53,  1, 69, 64,  1, 68, 50, 74,  1, 69,\n","        57, 50, 69,  1, 69, 57, 54, 74,  1, 72, 54, 67, 54,  1, 65, 54, 67, 55,\n","        54, 52, 69, 61, 74,  1, 63, 64, 67, 62, 50, 61,  7,  0, 69, 57, 50, 63,\n","        60,  1, 74, 64, 70,  1, 71, 54, 67, 74,  1, 62, 70, 52, 57,  9,  1, 43,\n","        57, 54, 74,  1, 72, 54, 67, 54,  1, 69, 57, 54,  1, 61, 50, 68, 69,  1,\n","        65, 54, 64, 65, 61, 54,  1, 74, 64, 70, 78, 53,  0, 54, 73, 65, 54, 52,\n","        69,  1, 69, 64,  1, 51, 54,  1, 58, 63, 71, 64, 61, 71, 54, 53,  1, 58,\n","        63,  1, 50, 63, 74, 69, 57, 58, 63, 56,  1, 68, 69, 67, 50, 63, 56, 54,\n","         1, 64, 67,  0, 62, 74, 68, 69, 54, 67, 58, 64, 70, 68,  7,  1, 51, 54,\n","        52, 50, 70, 68, 54,  1, 69, 57, 54, 74,  1, 59, 70, 68, 69,  1, 53, 58,\n","        53, 63, 78, 69,  1, 57, 64, 61, 53,  1, 72, 58, 69, 57,  1, 68, 70, 52,\n","        57,  0, 63, 64, 63, 68, 54, 63, 68, 54,  9,  0,  0, 36, 67,  9,  1, 27,\n","        70, 67, 68, 61, 54, 74,  1, 72, 50, 68,  1, 69, 57, 54,  1, 53, 58, 67,\n","        54, 52, 69, 64, 67,  1, 64, 55,  1, 50,  1, 55, 58, 67, 62,  1, 52, 50,\n","        61, 61, 54, 53,  0, 30, 67, 70, 63, 63, 58, 63, 56, 68,  7,  1, 72, 57,\n","        58, 52, 57,  1, 62, 50, 53, 54,  1, 53, 67, 58, 61, 61, 68,  9,  1, 31,\n","        54,  1, 72, 50, 68,  1, 50,  1, 51, 58, 56,  7,  1, 51, 54, 54, 55, 74,\n","         0, 62, 50, 63,  1, 72, 58, 69, 57,  1, 57, 50, 67, 53, 61, 74,  1, 50,\n","        63, 74,  1, 63, 54, 52, 60,  7,  1, 50, 61, 69, 57, 64, 70, 56, 57,  1,\n","        57, 54,  1, 53, 58, 53,  1, 57, 50, 71, 54,  1, 50,  0, 71, 54, 67, 74,\n","         1, 61, 50, 67, 56, 54,  1, 62, 70, 68, 69, 50, 52, 57, 54,  9,  1, 36,\n","        67, 68,  9,  1, 27, 70, 67, 68, 61, 54, 74,  1, 72, 50, 68,  1, 69, 57,\n","        58, 63,  1, 50, 63, 53,  0, 51, 61, 64, 63, 53, 54,  1, 50, 63, 53,  1,\n","        57, 50, 53,  1, 63, 54, 50, 67, 61, 74,  1, 69, 72, 58, 52, 54,  1, 69,\n","        57, 54,  1, 70, 68, 70, 50, 61,  1, 50, 62, 64, 70, 63, 69,  1, 64, 55,\n","         0, 63, 54, 52, 60,  7,  1, 72, 57, 58, 52, 57,  1, 52, 50, 62, 54,  1,\n","        58, 63,  1, 71, 54, 67, 74,  1, 70, 68, 54, 55, 70, 61,  1, 50, 68,  1,\n","        68, 57, 54,  1, 68, 65, 54, 63, 69,  1, 68, 64,  0, 62, 70, 52, 57,  1,\n","        64, 55,  1, 57, 54, 67,  1, 69, 58, 62, 54,  1, 52, 67, 50, 63, 58, 63,\n","        56,  1, 64, 71, 54, 67,  1, 56, 50, 67, 53, 54, 63,  1, 55, 54, 63, 52,\n","        54, 68,  7,  1, 68, 65, 74, 58, 63, 56,  0, 64, 63,  1, 69, 57, 54,  1,\n","        63, 54, 58, 56, 57, 51, 64, 67, 68,  9,  1, 43, 57, 54,  1, 27, 70, 67,\n","        68, 61, 54, 74,  1, 68,  1, 57, 50, 53,  1, 50,  1, 68, 62, 50, 61, 61,\n","         1, 68, 64, 63,  0, 52, 50, 61, 61, 54, 53,  1, 27, 70, 53, 61, 54, 74,\n","         1, 50, 63, 53,  1, 58, 63,  1, 69, 57, 54, 58, 67,  1, 64, 65, 58, 63,\n","        58, 64, 63,  1, 69, 57, 54, 67, 54,  1, 72, 50, 68,  1, 63, 64,  1, 55,\n","        58, 63, 54, 67,  0, 51, 64, 74,  1, 50, 63, 74, 72, 57, 54, 67, 54,  9,\n","         0,  0, 43, 57, 54,  1, 27, 70, 67, 68, 61, 54, 74, 68,  1, 57, 50, 53,\n","         1, 54, 71, 54, 67, 74, 69, 57, 58, 63, 56,  1, 69, 57, 54, 74,  1, 72,\n","        50, 63, 69, 54, 53,  7,  1, 51, 70, 69,  1, 69, 57, 54, 74,  0, 50, 61,\n","        68, 64,  1, 57, 50, 53,  1, 50,  1, 68, 54, 52, 67, 54, 69,  7,  1, 50,\n","        63, 53,  1, 69, 57, 54, 58, 67,  1, 56, 67, 54, 50, 69, 54, 68, 69,  1,\n","        55, 54, 50, 67,  1, 72, 50, 68,  1, 69, 57, 50, 69,  0, 68, 64, 62, 54,\n","        51, 64, 53, 74,  1, 72, 64, 70, 61, 53,  1, 53, 58, 68, 52, 64, 71, 54,\n","        67,  1, 58, 69,  9,  1, 43, 57, 54, 74,  1, 53, 58, 53, 63, 78, 69,  1,\n","        69, 57, 58, 63, 60,  1, 69, 57, 54, 74,  0, 52, 64, 70, 61, 53,  1, 51,\n","        54, 50, 67,  1, 58, 69,  1, 58, 55,  1, 50, 63, 74, 64, 63, 54,  1, 55,\n","        64, 70, 63, 53,  1, 64, 70, 69,  1, 50, 51, 64, 70, 69,  1, 69, 57, 54,\n","         1, 39, 64, 69, 69, 54, 67, 68,  9,  0, 36, 67, 68,  9,  1, 39, 64, 69,\n","        69, 54, 67,  1, 72, 50, 68,  1, 36, 67, 68,  9,  1, 27, 70, 67, 68, 61,\n","        54, 74, 78, 68,  1, 68, 58, 68, 69, 54, 67,  7,  1, 51, 70, 69,  1, 69,\n","        57, 54, 74,  1, 57, 50, 53, 63, 78, 69])\n"]}]},{"cell_type":"code","source":["#Let's now split up the data into train and validation sets\n","\n","n = int(0.9*len(data))\n","train_data = data[:n]\n","val_data = data[n:]"],"metadata":{"id":"wtYfoMMEAeBx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["block_size = 8\n","train_data[:block_size + 1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s7V320eSA1h1","executionInfo":{"status":"ok","timestamp":1711667099815,"user_tz":-330,"elapsed":30,"user":{"displayName":"Lokesh Dandumahanti","userId":"04322942827821170944"}},"outputId":"b512d3d7-f0df-45e9-b2dc-7e8993af6fbe"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([43, 31, 28,  1, 25, 38, 47,  1, 46])"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["x = train_data[:block_size]\n","y = train_data[1:block_size+1]\n","for t in range(block_size):\n","  context = x[:t+1]\n","  target = y[t]\n","  print(f\"when input is {context} the target: {target}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1de-iNl7A-3A","executionInfo":{"status":"ok","timestamp":1711667099815,"user_tz":-330,"elapsed":27,"user":{"displayName":"Lokesh Dandumahanti","userId":"04322942827821170944"}},"outputId":"cee27789-674d-42aa-f5bc-0447607d6afb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["when input is tensor([43]) the target: 31\n","when input is tensor([43, 31]) the target: 28\n","when input is tensor([43, 31, 28]) the target: 1\n","when input is tensor([43, 31, 28,  1]) the target: 25\n","when input is tensor([43, 31, 28,  1, 25]) the target: 38\n","when input is tensor([43, 31, 28,  1, 25, 38]) the target: 47\n","when input is tensor([43, 31, 28,  1, 25, 38, 47]) the target: 1\n","when input is tensor([43, 31, 28,  1, 25, 38, 47,  1]) the target: 46\n"]}]},{"cell_type":"markdown","source":["#4. Data loader : batches of chunks of data"],"metadata":{"id":"CGQAegsZQH4e"}},{"cell_type":"code","source":["torch.manual_seed(1337)\n","batch_size = 4\n","block_size = 8\n","\n","def get_batch(split):\n","  data = train_data if split == 'train' else val_data\n","  ix = torch.randint(len(data) - block_size, (batch_size,))\n","  x = torch.stack([data[i : i+block_size] for i in ix])\n","  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n","  return x,y\n","\n","xb, yb = get_batch('train')\n","print('inputs:')\n","print(xb.shape)\n","print(xb)\n","print('targets:')\n","print(yb.shape)\n","print(yb)\n","\n","print('-----')\n","\n","for b in range(batch_size):\n","  for t in range(block_size):\n","    context = xb[b, :t+1]\n","    target = yb[b,t]\n","    print(f\"when input is {context.tolist()} the target: {target}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v4zovBWGBd-N","executionInfo":{"status":"ok","timestamp":1711667101488,"user_tz":-330,"elapsed":6,"user":{"displayName":"Lokesh Dandumahanti","userId":"04322942827821170944"}},"outputId":"fafa59af-b01b-4aca-a277-f87b98f399af"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs:\n","torch.Size([4, 8])\n","tensor([[55,  1, 69, 57, 54,  0, 27, 70],\n","        [53,  1, 58, 69,  1, 64, 63, 69],\n","        [ 1, 65, 64, 58, 63, 69, 68,  1],\n","        [ 1, 64, 70, 69,  1, 58, 63, 69]])\n","targets:\n","torch.Size([4, 8])\n","tensor([[ 1, 69, 57, 54,  0, 27, 70, 67],\n","        [ 1, 58, 69,  1, 64, 63, 69, 64],\n","        [65, 64, 58, 63, 69, 68,  1, 55],\n","        [64, 70, 69,  1, 58, 63, 69, 64]])\n","-----\n","when input is [55] the target: 1\n","when input is [55, 1] the target: 69\n","when input is [55, 1, 69] the target: 57\n","when input is [55, 1, 69, 57] the target: 54\n","when input is [55, 1, 69, 57, 54] the target: 0\n","when input is [55, 1, 69, 57, 54, 0] the target: 27\n","when input is [55, 1, 69, 57, 54, 0, 27] the target: 70\n","when input is [55, 1, 69, 57, 54, 0, 27, 70] the target: 67\n","when input is [53] the target: 1\n","when input is [53, 1] the target: 58\n","when input is [53, 1, 58] the target: 69\n","when input is [53, 1, 58, 69] the target: 1\n","when input is [53, 1, 58, 69, 1] the target: 64\n","when input is [53, 1, 58, 69, 1, 64] the target: 63\n","when input is [53, 1, 58, 69, 1, 64, 63] the target: 69\n","when input is [53, 1, 58, 69, 1, 64, 63, 69] the target: 64\n","when input is [1] the target: 65\n","when input is [1, 65] the target: 64\n","when input is [1, 65, 64] the target: 58\n","when input is [1, 65, 64, 58] the target: 63\n","when input is [1, 65, 64, 58, 63] the target: 69\n","when input is [1, 65, 64, 58, 63, 69] the target: 68\n","when input is [1, 65, 64, 58, 63, 69, 68] the target: 1\n","when input is [1, 65, 64, 58, 63, 69, 68, 1] the target: 55\n","when input is [1] the target: 64\n","when input is [1, 64] the target: 70\n","when input is [1, 64, 70] the target: 69\n","when input is [1, 64, 70, 69] the target: 1\n","when input is [1, 64, 70, 69, 1] the target: 58\n","when input is [1, 64, 70, 69, 1, 58] the target: 63\n","when input is [1, 64, 70, 69, 1, 58, 63] the target: 69\n","when input is [1, 64, 70, 69, 1, 58, 63, 69] the target: 64\n"]}]},{"cell_type":"code","source":["print(xb)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B_POol24Dx_F","executionInfo":{"status":"ok","timestamp":1711667101869,"user_tz":-330,"elapsed":8,"user":{"displayName":"Lokesh Dandumahanti","userId":"04322942827821170944"}},"outputId":"51aafc0a-7b7d-4c57-d430-9d3b1fb41c4e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[55,  1, 69, 57, 54,  0, 27, 70],\n","        [53,  1, 58, 69,  1, 64, 63, 69],\n","        [ 1, 65, 64, 58, 63, 69, 68,  1],\n","        [ 1, 64, 70, 69,  1, 58, 63, 69]])\n"]}]},{"cell_type":"markdown","source":["#5. Simplest baseline: bigram language model, loss, generation\n","\n"],"metadata":{"id":"EWJ0Z1qpQllR"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","torch.manual_seed(1337)\n","\n","class BigramLanguageModel(nn.Module):\n","\n","    def __init__(self, vocab_size):\n","        super().__init__()\n","        # each token directly reads off the logits for the next token from a lookup table\n","        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n","\n","    def forward(self, idx, targets=None):\n","\n","        # idx and targets are both (B,T) tensor of integers\n","        logits = self.token_embedding_table(idx) # (B,T,C)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        # idx is (B, T) array of indices in the current context\n","        for _ in range(max_new_tokens):\n","            # get the predictions\n","            logits, loss = self(idx)\n","            # focus only on the last time step\n","            logits = logits[:, -1, :] # becomes (B, C)\n","            # apply softmax to get probabilities\n","            probs = F.softmax(logits, dim=-1) # (B, C)\n","            # sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n","            # append sampled index to the running sequence\n","            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n","        return idx\n","\n","m = BigramLanguageModel(vocab_size)\n","logits, loss = m(xb, yb)\n","print(logits.shape)\n","print(loss)\n","\n","print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ERBw3OZ8D2sh","executionInfo":{"status":"ok","timestamp":1711667111692,"user_tz":-330,"elapsed":10,"user":{"displayName":"Lokesh Dandumahanti","userId":"04322942827821170944"}},"outputId":"93da2acd-c9d2-4e90-f677-2bd82e6ff525"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 83])\n","tensor(4.3570, grad_fn=<NllLossBackward0>)\n","\n","44;5’L,OB)!,2W/()7fd Ckxae\"jR!Dtre0qUG“kB5(;QW•N\n","G—oAhhv\"SsGgj4yF-H9q:Ug■jgDJ4S11Bv!6Ulpnlitw;h’()KP\n"]}]},{"cell_type":"markdown","source":["#6. Training the Bigram Model\n"],"metadata":{"id":"2HrBq6TYfhNO"}},{"cell_type":"code","source":["# create a PyTorch optimizer\n","optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"],"metadata":{"id":"pZQ1l4KnRSex"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 32\n","for steps in range(100): # increase number of steps for good results...\n","\n","    # sample a batch of data\n","    xb, yb = get_batch('train')\n","\n","    # evaluate the loss\n","    logits, loss = m(xb, yb)\n","    optimizer.zero_grad(set_to_none=True)\n","    loss.backward()\n","    optimizer.step()\n","\n","print(loss.item())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9f_7r8f9RchN","executionInfo":{"status":"ok","timestamp":1711667120799,"user_tz":-330,"elapsed":605,"user":{"displayName":"Lokesh Dandumahanti","userId":"04322942827821170944"}},"outputId":"ade09bcd-f471-4dc0-c576-077bfe09ed8b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["4.714195728302002\n"]}]},{"cell_type":"markdown","source":["#7. Port our code to a script"],"metadata":{"id":"9Il3MOkxh9Gu"}},{"cell_type":"code","source":["print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0mDRWnnmRg3Q","executionInfo":{"status":"ok","timestamp":1711667122814,"user_tz":-330,"elapsed":9,"user":{"displayName":"Lokesh Dandumahanti","userId":"04322942827821170944"}},"outputId":"7c4a8f9d-aacd-49bf-e463-36cba75ff5ed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n",")T\n","UT6t•-!K?n-Jqn3nIrtE\\MY,3nb849Gy'hyNb1!?ZWlElwlwp■Lgnsod‘,J\"4E/o'Z,lPA8cT‘QJ;l4GFll:qL!hztmoT!RJtKPxx:cJ!aCnm-H4Pxbnw?- k!ERFk\"y0n-(!NkWYK7hC2NY■xO75TMB;6,Z'Lz C\n","egMj2pf—sOgrYW9'—M l4•12\"yF6orjlaO.”y'5RSEQ7ZLr6z2 k‘WIkPl0l(lC\n","lW928?7s0a“pntsESg?5■‘n6OFaKao“UL’rLP;;n-oJjOESGsVT,JgsJl-RCh!7IJwZ\n","akSn\n",":xSW-J.uS”Vq)nh:e0■DZ■I’e,P)d\n","w‘nGpmx, WeZW(BR)\"2Yj7vLS)TFy‘”tm N\n","5AbkFj•k.,ZKy3FKH49RRMOYMs Na“p8Nx8NI3”■M?OBzK84V”tY'—ISuqGsfC■1FJHQ5W/Pz.LrKsz):Y5vor5K0nt84s”:x‘/gN3ntBR\"1qT2NwZnx!vKZE3N.w6r?f?T \n"]}]},{"cell_type":"markdown","source":["#8. The trick in self-attention: matrix multiply as weight aggregation"],"metadata":{"id":"tPb9rcNVRktD"}},{"cell_type":"code","source":["# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n","torch.manual_seed(42)\n","a = torch.tril(torch.ones(3, 3))\n","a = a / torch.sum(a, 1, keepdim=True)\n","b = torch.randint(0,10,(3,2)).float()\n","c = a @ b\n","print('a=')\n","print(a)\n","print('--')\n","print('b=')\n","print(b)\n","print('--')\n","print('c=')\n","print(c)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AcwiGHGcSLoI","executionInfo":{"status":"ok","timestamp":1711667127451,"user_tz":-330,"elapsed":395,"user":{"displayName":"Lokesh Dandumahanti","userId":"04322942827821170944"}},"outputId":"726d939b-14c4-484f-ed98-25a3f2c2712a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["a=\n","tensor([[1.0000, 0.0000, 0.0000],\n","        [0.5000, 0.5000, 0.0000],\n","        [0.3333, 0.3333, 0.3333]])\n","--\n","b=\n","tensor([[2., 7.],\n","        [6., 4.],\n","        [6., 5.]])\n","--\n","c=\n","tensor([[2.0000, 7.0000],\n","        [4.0000, 5.5000],\n","        [4.6667, 5.3333]])\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"AHRcjq1NiT_M"}},{"cell_type":"code","source":["# consider the following toy example:\n","\n","torch.manual_seed(1337)\n","B,T,C = 4,8,2 # batch, time, channels\n","x = torch.randn(B,T,C)\n","x.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TkPhcJsvSRff","executionInfo":{"status":"ok","timestamp":1711667127818,"user_tz":-330,"elapsed":9,"user":{"displayName":"Lokesh Dandumahanti","userId":"04322942827821170944"}},"outputId":"6d0187d5-3267-494c-c9ce-c54ccf5bd7d1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4, 8, 2])"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["# We want x[b,t] = mean_{i<=t} x[b,i]\n","xbow = torch.zeros((B,T,C))\n","for b in range(B):\n","    for t in range(T):\n","        xprev = x[b,:t+1] # (t,C)\n","        xbow[b,t] = torch.mean(xprev, 0)"],"metadata":{"id":"T-SCPrvMSV87"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#9. V2: Using matrix multiplication"],"metadata":{"id":"zc7x_2ZOSwds"}},{"cell_type":"code","source":["# version 2: using matrix multiply for a weighted aggregation\n","wei = torch.tril(torch.ones(T, T))\n","wei = wei / wei.sum(1, keepdim=True)\n","xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n","torch.allclose(xbow, xbow2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Evfz-ziSpCY","executionInfo":{"status":"ok","timestamp":1711667130753,"user_tz":-330,"elapsed":370,"user":{"displayName":"Lokesh Dandumahanti","userId":"04322942827821170944"}},"outputId":"c6a3ae89-7026-4f29-bdb2-46c55a9a6c73"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","source":["#10. V3. Adding softmax"],"metadata":{"id":"HCeAMYnKS7iF"}},{"cell_type":"code","source":["# version 3: use Softmax\n","tril = torch.tril(torch.ones(T, T))\n","wei = torch.zeros((T,T))\n","wei = wei.masked_fill(tril == 0, float('-inf'))\n","wei = F.softmax(wei, dim=-1)\n","xbow3 = wei @ x\n","torch.allclose(xbow, xbow3)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ADZyCNR7THtR","executionInfo":{"status":"ok","timestamp":1711667134006,"user_tz":-330,"elapsed":716,"user":{"displayName":"Lokesh Dandumahanti","userId":"04322942827821170944"}},"outputId":"2c9071db-0b46-4732-dba7-98cc04fb231e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","source":["#12. V4. self attention"],"metadata":{"id":"7Wgr06ZLTNvP"}},{"cell_type":"code","source":["# version 4: self-attention!\n","torch.manual_seed(1337)\n","B,T,C = 4,8,32 # batch, time, channels\n","x = torch.randn(B,T,C)\n","\n","# let's see a single Head perform self-attention\n","head_size = 16\n","key = nn.Linear(C, head_size, bias=False)\n","query = nn.Linear(C, head_size, bias=False)\n","value = nn.Linear(C, head_size, bias=False)\n","k = key(x)   # (B, T, 16)\n","q = query(x) # (B, T, 16)\n","wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n","\n","tril = torch.tril(torch.ones(T, T))\n","#wei = torch.zeros((T,T))\n","wei = wei.masked_fill(tril == 0, float('-inf'))\n","wei = F.softmax(wei, dim=-1)\n","\n","v = value(x)\n","out = wei @ v\n","#out = wei @ x\n","\n","out.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zzUOVIQ8Te8K","executionInfo":{"status":"ok","timestamp":1711667140443,"user_tz":-330,"elapsed":414,"user":{"displayName":"Lokesh Dandumahanti","userId":"04322942827821170944"}},"outputId":"772e2ba4-9021-4999-83a4-1dcabb70aaf7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4, 8, 16])"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["wei[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O4LznjneTjur","executionInfo":{"status":"ok","timestamp":1711667140919,"user_tz":-330,"elapsed":6,"user":{"displayName":"Lokesh Dandumahanti","userId":"04322942827821170944"}},"outputId":"0b5ef1a2-5a31-41ed-9ec0-01ac69c033cf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n","        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n","        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n","        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n","       grad_fn=<SelectBackward0>)"]},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","source":["#13. n1: attention as communication\n"],"metadata":{"id":"evGWJ5IQTNfh"}},{"cell_type":"markdown","source":["Notes:\n","\n","1. Attention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n","2. There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n","3. Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n","4.In an \"encoder\" attention block just delete the single line that does masking with tril, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n","5. \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n","6. \"Scaled\" attention additional divides wei by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"],"metadata":{"id":"-8vhwrkKT1OL"}},{"cell_type":"markdown","source":["#14. Inserting a single self-attention block to our network"],"metadata":{"id":"5kFVjkoXUWct"}},{"cell_type":"code","source":["k = torch.randn(B,T,head_size)\n","q = torch.randn(B,T,head_size)\n","wei = q @ k.transpose(-2, -1) * head_size**-0.5"],"metadata":{"id":"pjkLK0DtTxvj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["k.var()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BiiEcX2_UrTd","executionInfo":{"status":"ok","timestamp":1711669503963,"user_tz":-330,"elapsed":26,"user":{"displayName":"Lokesh Dandumahanti","userId":"04322942827821170944"}},"outputId":"4d79dd70-a10e-49c1-ade7-d5da3f93f5ad"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.9831)"]},"metadata":{},"execution_count":71}]},{"cell_type":"code","source":["q.var()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sc-hkTO4VIzS","executionInfo":{"status":"ok","timestamp":1711669503965,"user_tz":-330,"elapsed":26,"user":{"displayName":"Lokesh Dandumahanti","userId":"04322942827821170944"}},"outputId":"e8334d1a-479e-43d7-d014-68adfd55f5a9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1.0026)"]},"metadata":{},"execution_count":72}]},{"cell_type":"code","source":["wei.var()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"68iTSVfuVP2Z","executionInfo":{"status":"ok","timestamp":1711669503965,"user_tz":-330,"elapsed":25,"user":{"displayName":"Lokesh Dandumahanti","userId":"04322942827821170944"}},"outputId":"34bc1764-8875-4513-ce29-c74b69d688ea"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1.0456)"]},"metadata":{},"execution_count":73}]},{"cell_type":"code","source":["torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BDSch3WwVSdU","executionInfo":{"status":"ok","timestamp":1711669503965,"user_tz":-330,"elapsed":23,"user":{"displayName":"Lokesh Dandumahanti","userId":"04322942827821170944"}},"outputId":"79b84777-c1dd-421d-9bfd-02e428c68d1f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"]},"metadata":{},"execution_count":74}]},{"cell_type":"code","source":["torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2wnWmXokVh_-","executionInfo":{"status":"ok","timestamp":1711669503965,"user_tz":-330,"elapsed":22,"user":{"displayName":"Lokesh Dandumahanti","userId":"04322942827821170944"}},"outputId":"3938692e-4905-47f3-b479-0cac3fe86ce2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"]},"metadata":{},"execution_count":75}]},{"cell_type":"code","source":["class LayerNorm1d: # (used to be BatchNorm1d)\n","\n","  def __init__(self, dim, eps=1e-5, momentum=0.1):\n","    self.eps = eps\n","    self.gamma = torch.ones(dim)\n","    self.beta = torch.zeros(dim)\n","\n","  def __call__(self, x):\n","    # calculate the forward pass\n","    xmean = x.mean(1, keepdim=True) # batch mean\n","    xvar = x.var(1, keepdim=True) # batch variance\n","    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n","    self.out = self.gamma * xhat + self.beta\n","    return self.out\n","\n","  def parameters(self):\n","    return [self.gamma, self.beta]\n","\n","torch.manual_seed(1337)\n","module = LayerNorm1d(100)\n","x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n","x = module(x)\n","x.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o1BVAqFHVkof","executionInfo":{"status":"ok","timestamp":1711669503965,"user_tz":-330,"elapsed":21,"user":{"displayName":"Lokesh Dandumahanti","userId":"04322942827821170944"}},"outputId":"3603c5ef-fc6a-4ead-b5e0-8c27546e0e58"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([32, 100])"]},"metadata":{},"execution_count":76}]},{"cell_type":"code","source":["x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oMRL8gvEVpxY","executionInfo":{"status":"ok","timestamp":1711669503966,"user_tz":-330,"elapsed":21,"user":{"displayName":"Lokesh Dandumahanti","userId":"04322942827821170944"}},"outputId":"90fbd9b2-5afe-4ddb-92d2-8714beeb7f02"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(0.1469), tensor(0.8803))"]},"metadata":{},"execution_count":77}]},{"cell_type":"code","source":["# French to English translation example:\n","\n","# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n","# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>\n","\n"],"metadata":{"id":"ML5-YitWVtrZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#"],"metadata":{"id":"dKIlukc1WIu8"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","\n","# hyperparameters\n","batch_size = 16 # how many independent sequences will we process in parallel?\n","block_size = 32 # what is the maximum context length for predictions?\n","max_iters = 20000\n","eval_interval = 100\n","learning_rate = 1e-3\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","eval_iters = 200\n","n_embd = 64\n","n_head = 4\n","n_layer = 4\n","dropout = 0.0\n","# ------------\n","\n","torch.manual_seed(1337)\n","\n","# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n","with open('sorcerers-stone.txt', 'r', encoding='utf-8') as f:\n","    text = f.read()\n","\n","# here are all the unique characters that occur in this text\n","chars = sorted(list(set(text)))\n","vocab_size = len(chars)\n","# create a mapping from characters to integers\n","stoi = { ch:i for i,ch in enumerate(chars) }\n","itos = { i:ch for i,ch in enumerate(chars) }\n","encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n","decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n","\n","# Train and test splits\n","data = torch.tensor(encode(text), dtype=torch.long)\n","n = int(0.9*len(data)) # first 90% will be train, rest val\n","train_data = data[:n]\n","val_data = data[n:]\n","\n","# data loading\n","def get_batch(split):\n","    # generate a small batch of data of inputs x and targets y\n","    data = train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([data[i:i+block_size] for i in ix])\n","    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n","    x, y = x.to(device), y.to(device)\n","    return x, y\n","\n","@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        for k in range(eval_iters):\n","            X, Y = get_batch(split)\n","            logits, loss = model(X, Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out\n","\n","class Head(nn.Module):\n","    \"\"\" one head of self-attention \"\"\"\n","\n","    def __init__(self, head_size):\n","        super().__init__()\n","        self.key = nn.Linear(n_embd, head_size, bias=False)\n","        self.query = nn.Linear(n_embd, head_size, bias=False)\n","        self.value = nn.Linear(n_embd, head_size, bias=False)\n","        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        B,T,C = x.shape\n","        k = self.key(x)   # (B,T,C)\n","        q = self.query(x) # (B,T,C)\n","        # compute attention scores (\"affinities\")\n","        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n","        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n","        wei = F.softmax(wei, dim=-1) # (B, T, T)\n","        wei = self.dropout(wei)\n","        # perform the weighted aggregation of the values\n","        v = self.value(x) # (B,T,C)\n","        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n","        return out\n","\n","class MultiHeadAttention(nn.Module):\n","    \"\"\" multiple heads of self-attention in parallel \"\"\"\n","\n","    def __init__(self, num_heads, head_size):\n","        super().__init__()\n","        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n","        self.proj = nn.Linear(n_embd, n_embd)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        out = torch.cat([h(x) for h in self.heads], dim=-1)\n","        out = self.dropout(self.proj(out))\n","        return out\n","\n","class FeedFoward(nn.Module):\n","    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n","\n","    def __init__(self, n_embd):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(n_embd, 4 * n_embd),\n","            nn.ReLU(),\n","            nn.Linear(4 * n_embd, n_embd),\n","            nn.Dropout(dropout),\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","class Block(nn.Module):\n","    \"\"\" Transformer block: communication followed by computation \"\"\"\n","\n","    def __init__(self, n_embd, n_head):\n","        # n_embd: embedding dimension, n_head: the number of heads we'd like\n","        super().__init__()\n","        head_size = n_embd // n_head\n","        self.sa = MultiHeadAttention(n_head, head_size)\n","        self.ffwd = FeedFoward(n_embd)\n","        self.ln1 = nn.LayerNorm(n_embd)\n","        self.ln2 = nn.LayerNorm(n_embd)\n","\n","    def forward(self, x):\n","        x = x + self.sa(self.ln1(x))\n","        x = x + self.ffwd(self.ln2(x))\n","        return x\n","\n","# super simple bigram model\n","class BigramLanguageModel(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        # each token directly reads off the logits for the next token from a lookup table\n","        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n","        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n","        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n","        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n","        self.lm_head = nn.Linear(n_embd, vocab_size)\n","\n","    def forward(self, idx, targets=None):\n","        B, T = idx.shape\n","\n","        # idx and targets are both (B,T) tensor of integers\n","        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n","        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n","        x = tok_emb + pos_emb # (B,T,C)\n","        x = self.blocks(x) # (B,T,C)\n","        x = self.ln_f(x) # (B,T,C)\n","        logits = self.lm_head(x) # (B,T,vocab_size)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        # idx is (B, T) array of indices in the current context\n","        for _ in range(max_new_tokens):\n","            # crop idx to the last block_size tokens\n","            idx_cond = idx[:, -block_size:]\n","            # get the predictions\n","            logits, loss = self(idx_cond)\n","            # focus only on the last time step\n","            logits = logits[:, -1, :] # becomes (B, C)\n","            # apply softmax to get probabilities\n","            probs = F.softmax(logits, dim=-1) # (B, C)\n","            # sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n","            # append sampled index to the running sequence\n","            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n","        return idx"],"metadata":{"id":"VPCwf6jqWRMS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = BigramLanguageModel()\n","m = model.to(device)\n","# print the number of parameters in the model\n","print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n","\n","# create a PyTorch optimizer\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","for iter in range(max_iters):\n","\n","    # every once in a while evaluate the loss on train and val sets\n","    if iter % eval_interval == 0 or iter == max_iters - 1:\n","        losses = estimate_loss()\n","        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","    # sample a batch of data\n","    xb, yb = get_batch('train')\n","\n","    # evaluate the loss\n","    logits, loss = model(xb, yb)\n","    optimizer.zero_grad(set_to_none=True)\n","    loss.backward()\n","    optimizer.step()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W35WXSLdbasw","executionInfo":{"status":"ok","timestamp":1711671733814,"user_tz":-330,"elapsed":325234,"user":{"displayName":"Lokesh Dandumahanti","userId":"04322942827821170944"}},"outputId":"a3786a24-e8ca-45d5-dde5-b421a8be5931"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.212051 M parameters\n","step 0: train loss 4.6292, val loss 4.6379\n","step 100: train loss 2.6828, val loss 2.6865\n","step 200: train loss 2.5206, val loss 2.5135\n","step 300: train loss 2.4815, val loss 2.4703\n","step 400: train loss 2.3916, val loss 2.3789\n","step 500: train loss 2.2997, val loss 2.2922\n","step 600: train loss 2.2432, val loss 2.2296\n","step 700: train loss 2.1636, val loss 2.1628\n","step 800: train loss 2.1059, val loss 2.1173\n","step 900: train loss 2.0570, val loss 2.0616\n","step 1000: train loss 2.0136, val loss 2.0154\n","step 1100: train loss 1.9834, val loss 2.0010\n","step 1200: train loss 1.9532, val loss 1.9684\n","step 1300: train loss 1.9211, val loss 1.9338\n","step 1400: train loss 1.9033, val loss 1.9226\n","step 1500: train loss 1.8629, val loss 1.8875\n","step 1600: train loss 1.8461, val loss 1.8589\n","step 1700: train loss 1.8239, val loss 1.8645\n","step 1800: train loss 1.8100, val loss 1.8295\n","step 1900: train loss 1.7784, val loss 1.7940\n","step 2000: train loss 1.7735, val loss 1.7970\n","step 2100: train loss 1.7624, val loss 1.7912\n","step 2200: train loss 1.7521, val loss 1.7715\n","step 2300: train loss 1.7401, val loss 1.7721\n","step 2400: train loss 1.7175, val loss 1.7354\n","step 2500: train loss 1.7142, val loss 1.7373\n","step 2600: train loss 1.7057, val loss 1.7274\n","step 2700: train loss 1.6830, val loss 1.7275\n","step 2800: train loss 1.6877, val loss 1.7109\n","step 2900: train loss 1.6787, val loss 1.7055\n","step 3000: train loss 1.6721, val loss 1.6992\n","step 3100: train loss 1.6616, val loss 1.6937\n","step 3200: train loss 1.6599, val loss 1.6961\n","step 3300: train loss 1.6455, val loss 1.6680\n","step 3400: train loss 1.6428, val loss 1.6835\n","step 3500: train loss 1.6300, val loss 1.6757\n","step 3600: train loss 1.6273, val loss 1.6760\n","step 3700: train loss 1.6237, val loss 1.6672\n","step 3800: train loss 1.6148, val loss 1.6611\n","step 3900: train loss 1.6075, val loss 1.6452\n","step 4000: train loss 1.6054, val loss 1.6416\n","step 4100: train loss 1.6121, val loss 1.6528\n","step 4200: train loss 1.5852, val loss 1.6198\n","step 4300: train loss 1.5873, val loss 1.6345\n","step 4400: train loss 1.5999, val loss 1.6352\n","step 4500: train loss 1.5810, val loss 1.6201\n","step 4600: train loss 1.5771, val loss 1.6095\n","step 4700: train loss 1.5710, val loss 1.6124\n","step 4800: train loss 1.5633, val loss 1.6075\n","step 4900: train loss 1.5462, val loss 1.6024\n","step 5000: train loss 1.5468, val loss 1.6165\n","step 5100: train loss 1.5581, val loss 1.5928\n","step 5200: train loss 1.5567, val loss 1.5986\n","step 5300: train loss 1.5509, val loss 1.5839\n","step 5400: train loss 1.5475, val loss 1.5934\n","step 5500: train loss 1.5398, val loss 1.6010\n","step 5600: train loss 1.5373, val loss 1.5917\n","step 5700: train loss 1.5379, val loss 1.5837\n","step 5800: train loss 1.5254, val loss 1.5892\n","step 5900: train loss 1.5151, val loss 1.5772\n","step 6000: train loss 1.5217, val loss 1.5894\n","step 6100: train loss 1.5230, val loss 1.5877\n","step 6200: train loss 1.5197, val loss 1.5754\n","step 6300: train loss 1.5203, val loss 1.5742\n","step 6400: train loss 1.5261, val loss 1.5817\n","step 6500: train loss 1.4977, val loss 1.5732\n","step 6600: train loss 1.5034, val loss 1.5695\n","step 6700: train loss 1.4944, val loss 1.5685\n","step 6800: train loss 1.5035, val loss 1.5645\n","step 6900: train loss 1.5006, val loss 1.5689\n","step 7000: train loss 1.4951, val loss 1.5626\n","step 7100: train loss 1.4927, val loss 1.5614\n","step 7200: train loss 1.4892, val loss 1.5682\n","step 7300: train loss 1.4946, val loss 1.5674\n","step 7400: train loss 1.4906, val loss 1.5591\n","step 7500: train loss 1.4868, val loss 1.5462\n","step 7600: train loss 1.4734, val loss 1.5486\n","step 7700: train loss 1.4673, val loss 1.5436\n","step 7800: train loss 1.4806, val loss 1.5517\n","step 7900: train loss 1.4781, val loss 1.5533\n","step 8000: train loss 1.4714, val loss 1.5400\n","step 8100: train loss 1.4753, val loss 1.5455\n","step 8200: train loss 1.4546, val loss 1.5455\n","step 8300: train loss 1.4549, val loss 1.5400\n","step 8400: train loss 1.4693, val loss 1.5409\n","step 8500: train loss 1.4602, val loss 1.5478\n","step 8600: train loss 1.4558, val loss 1.5519\n","step 8700: train loss 1.4630, val loss 1.5421\n","step 8800: train loss 1.4615, val loss 1.5478\n","step 8900: train loss 1.4570, val loss 1.5295\n","step 9000: train loss 1.4524, val loss 1.5302\n","step 9100: train loss 1.4555, val loss 1.5414\n","step 9200: train loss 1.4547, val loss 1.5342\n","step 9300: train loss 1.4525, val loss 1.5311\n","step 9400: train loss 1.4568, val loss 1.5280\n","step 9500: train loss 1.4416, val loss 1.5268\n","step 9600: train loss 1.4379, val loss 1.5289\n","step 9700: train loss 1.4531, val loss 1.5370\n","step 9800: train loss 1.4454, val loss 1.5163\n","step 9900: train loss 1.4453, val loss 1.5174\n","step 10000: train loss 1.4370, val loss 1.5263\n","step 10100: train loss 1.4445, val loss 1.5240\n","step 10200: train loss 1.4405, val loss 1.5311\n","step 10300: train loss 1.4327, val loss 1.5248\n","step 10400: train loss 1.4273, val loss 1.5218\n","step 10500: train loss 1.4378, val loss 1.5254\n","step 10600: train loss 1.4258, val loss 1.5193\n","step 10700: train loss 1.4248, val loss 1.5251\n","step 10800: train loss 1.4158, val loss 1.5261\n","step 10900: train loss 1.4363, val loss 1.5222\n","step 11000: train loss 1.4327, val loss 1.5131\n","step 11100: train loss 1.4123, val loss 1.5186\n","step 11200: train loss 1.4249, val loss 1.5239\n","step 11300: train loss 1.4249, val loss 1.5055\n","step 11400: train loss 1.4251, val loss 1.5267\n","step 11500: train loss 1.4220, val loss 1.5223\n","step 11600: train loss 1.4225, val loss 1.5168\n","step 11700: train loss 1.4166, val loss 1.5236\n","step 11800: train loss 1.4262, val loss 1.5155\n","step 11900: train loss 1.4056, val loss 1.5054\n","step 12000: train loss 1.4096, val loss 1.5161\n","step 12100: train loss 1.4093, val loss 1.5101\n","step 12200: train loss 1.4149, val loss 1.5215\n","step 12300: train loss 1.4165, val loss 1.5008\n","step 12400: train loss 1.4025, val loss 1.5072\n","step 12500: train loss 1.4007, val loss 1.5092\n","step 12600: train loss 1.4136, val loss 1.5213\n","step 12700: train loss 1.3973, val loss 1.5110\n","step 12800: train loss 1.4080, val loss 1.5074\n","step 12900: train loss 1.3923, val loss 1.5011\n","step 13000: train loss 1.4115, val loss 1.5080\n","step 13100: train loss 1.4084, val loss 1.4941\n","step 13200: train loss 1.4177, val loss 1.4989\n","step 13300: train loss 1.4006, val loss 1.5078\n","step 13400: train loss 1.3932, val loss 1.4997\n","step 13500: train loss 1.3979, val loss 1.5025\n","step 13600: train loss 1.3993, val loss 1.4948\n","step 13700: train loss 1.3924, val loss 1.4959\n","step 13800: train loss 1.4051, val loss 1.5079\n","step 13900: train loss 1.3978, val loss 1.5034\n","step 14000: train loss 1.3976, val loss 1.5015\n","step 14100: train loss 1.3934, val loss 1.5075\n","step 14200: train loss 1.3747, val loss 1.4983\n","step 14300: train loss 1.3823, val loss 1.5043\n","step 14400: train loss 1.3923, val loss 1.4935\n","step 14500: train loss 1.3947, val loss 1.4998\n","step 14600: train loss 1.3856, val loss 1.4990\n","step 14700: train loss 1.3811, val loss 1.4912\n","step 14800: train loss 1.3828, val loss 1.5029\n","step 14900: train loss 1.3800, val loss 1.5019\n","step 15000: train loss 1.3824, val loss 1.5061\n","step 15100: train loss 1.3786, val loss 1.5005\n","step 15200: train loss 1.3780, val loss 1.4973\n","step 15300: train loss 1.3850, val loss 1.4924\n","step 15400: train loss 1.3752, val loss 1.4926\n","step 15500: train loss 1.3753, val loss 1.5013\n","step 15600: train loss 1.3777, val loss 1.5061\n","step 15700: train loss 1.3688, val loss 1.4871\n","step 15800: train loss 1.3827, val loss 1.4870\n","step 15900: train loss 1.3666, val loss 1.4863\n","step 16000: train loss 1.3779, val loss 1.4879\n","step 16100: train loss 1.3689, val loss 1.4837\n","step 16200: train loss 1.3853, val loss 1.5000\n","step 16300: train loss 1.3694, val loss 1.4940\n","step 16400: train loss 1.3779, val loss 1.4906\n","step 16500: train loss 1.3745, val loss 1.4987\n","step 16600: train loss 1.3729, val loss 1.4909\n","step 16700: train loss 1.3675, val loss 1.4835\n","step 16800: train loss 1.3750, val loss 1.5002\n","step 16900: train loss 1.3705, val loss 1.4930\n","step 17000: train loss 1.3738, val loss 1.4922\n","step 17100: train loss 1.3681, val loss 1.4859\n","step 17200: train loss 1.3679, val loss 1.4848\n","step 17300: train loss 1.3627, val loss 1.5050\n","step 17400: train loss 1.3678, val loss 1.4952\n","step 17500: train loss 1.3583, val loss 1.4896\n","step 17600: train loss 1.3706, val loss 1.4831\n","step 17700: train loss 1.3608, val loss 1.4805\n","step 17800: train loss 1.3591, val loss 1.4784\n","step 17900: train loss 1.3638, val loss 1.4910\n","step 18000: train loss 1.3454, val loss 1.4754\n","step 18100: train loss 1.3615, val loss 1.4872\n","step 18200: train loss 1.3593, val loss 1.4822\n","step 18300: train loss 1.3556, val loss 1.4908\n","step 18400: train loss 1.3552, val loss 1.4990\n","step 18500: train loss 1.3486, val loss 1.4880\n","step 18600: train loss 1.3616, val loss 1.4852\n","step 18700: train loss 1.3477, val loss 1.4923\n","step 18800: train loss 1.3530, val loss 1.4912\n","step 18900: train loss 1.3494, val loss 1.4918\n","step 19000: train loss 1.3604, val loss 1.4917\n","step 19100: train loss 1.3511, val loss 1.4858\n","step 19200: train loss 1.3565, val loss 1.4725\n","step 19300: train loss 1.3511, val loss 1.4875\n","step 19400: train loss 1.3502, val loss 1.4815\n","step 19500: train loss 1.3560, val loss 1.4905\n","step 19600: train loss 1.3512, val loss 1.4869\n","step 19700: train loss 1.3578, val loss 1.4861\n","step 19800: train loss 1.3474, val loss 1.4760\n","step 19900: train loss 1.3495, val loss 1.4872\n","step 19999: train loss 1.3269, val loss 1.4709\n"]}]},{"cell_type":"markdown","source":["#15. Saving model"],"metadata":{"id":"vr0327NQqj_l"}},{"cell_type":"code","source":["torch.save(model.state_dict(),'/content/drive/MyDrive/saved_models/modelboy1.h5')\n"],"metadata":{"id":"R3H0DtRmq86k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"0RBJvXTyqj1i"}},{"cell_type":"markdown","source":[],"metadata":{"id":"CW3b5B4SqSWh"}},{"cell_type":"markdown","source":["#16. Prompting and output"],"metadata":{"id":"5NuzjhaOaSUC"}},{"cell_type":"markdown","source":[],"metadata":{"id":"L7VjKMqLrWWC"}},{"cell_type":"code","source":["model = BigramLanguageModel()\n","state_dict = torch.load('/content/drive/MyDrive/saved_models/modelboy1.h5')\n","\n","# Load the state dictionary into the model\n","model.load_state_dict(state_dict)\n","\n","# Set the model to evaluation mode\n","model.eval()\n","\n","# Tokenize the starting prompt\n","prompt = \"what is the difference between harry and hagrid\"\n","tokens = torch.tensor(encode(prompt), dtype=torch.long).unsqueeze(0)\n","\n","\n","# Generate the conversation\n","max_tokens = 2000\n","word_count = 0\n","\n","  # Maximum number of tokens to generate\n","for _ in range(max_tokens):\n","    # Generate the next token\n","    with torch.no_grad():\n","        output = model.generate(tokens, max_new_tokens=1)\n","    new_token = output[:, -1].item()\n","\n","    # Convert the new token to a character and append to the prompt\n","    # Convert the new token to a character and append to the prompt\n","    new_char = itos[new_token]\n","    if new_char == '':\n","      word_count += 1\n","    tokens = torch.cat((tokens, torch.tensor([[new_token]])), dim=1)\n","\n","    if word_count >= 2000:\n","      break\n","    # Print the conversation so far\n","# Convert the token indices to characters and join them into a string\n","generated_text = ''.join([itos[idx] for idx in tokens.squeeze().tolist()])\n","\n","# Print the conversation so far\n","print(generated_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DnXZbHZiYcZ8","executionInfo":{"status":"ok","timestamp":1711674162979,"user_tz":-330,"elapsed":23493,"user":{"displayName":"Lokesh Dandumahanti","userId":"04322942827821170944"}},"outputId":"af6edd3c-82f6-4316-d082-0eae4e228e93"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["what is the difference between harry and hagrid of her\n","seen in clutching the Gryffial the Restrictaker was open down it on thousand Ron manother worse.\n","\n","“Could told hope?” Hagrid\n","shoudy are them in everything\n","he could ever tomor and fire left once, Uncle Verty told here, I’m\n","golden a ticked as choosions it. He had\n","reached.\n","\n","“Yeah,” said Wood. “have you, taring over his\n","legs with hide? I was like that evelops over the match,\n","rubbed at made. A your gamekeen.\n","\n","“About it all, but Harry?”\n","\n","Harry hurried cail\n","way. “Answeups, and he couldn’t see her good; a great now thin’. exportictaking made something. Cheel. It don’ waits\n","toward, Harry much me first and clutching it, but\n","at last the game ookly, rapping their was\n","acrat.\n","\n","On the planes sort’s he end oujn which admiral possible coming were\n","let have to steal sleeks, a Stain\n","jearty? But it one told tell kitchen his front he lost mind\n","without — ”\n","\n","\n","\n","\n","\n","\n","\n","At liugh of the others toward Harry and\n","gloones could have laughed toward crack to wait, as though, he’d\n","nothing purs,” Harry began.\n","\n","Start slouded to his pobraps oper of the mirror. Nuse\n","reared. His five mly down the boy\n","whise, so pret inside Christmuch that.\n","\n","“At’s that he lips of the dusk eyes one, becon at thinking of ter leaving. Harry same?\n","\n","“Missin,” said Harry, at\n","made stallowed his rong against over ears. That yeh sneak who was\n","tune.” Hermione, nice?”\n","\n","“But his like Professor Hagrid\n","saves, because you’ll read have ditch owl off to\n","see\n","hurdled, taking yers out of his mind. He rewells sortain.\n","\n","“Not flood enordly got never Quirrell in — a unmiogs to go on a trick he night Harry one saying his cracking\n","could storm him and the gaspering told up I’ve read brought, this something,” said,\n","looking a\n","Constodoe more. It does — he’s already noticed\n","a lozing in the broom.\n","\n","“Fired a little for teacher’s life yeh’ll see in minutes High growled a special.\n","\n","\n","\n","\n","\n","“Heys astrick Will?\n","\n","as went started in brokdom that the truthing\n","door, does .., Mr. Dlife Voldemot she met I think on his\n","nobody one.\n","\n","“How stood — anyway, the glass, Har\n","what is the difference between harry and hagrid it on\n","the next weekly left on his four less. Harry\n","sat dived as high ligh in shot was no in about the cration that that purse was\n","as great Wizating, a Salches. When you same’s a\n","kight through it,”\n","said Slitting his back tarm.\n","\n","“I’d just and Ron was\n","with Harry’s stabbler a more who p-pleas, cheered and it out his\n","spell up it out once.\n","\n","\n","\n","\n","\n","\n","9 courage in the owl had half-studying.\n","\n","“As Harry saw without of course, seizeloss too\n","was Ron side.\n","\n","“ And something him in a\n","rught the very despeost\n","turn if his huggesting at be a lamglazily\n","witching ... totted to his feet in the\n","did like through the pause his coat. “Duley. Hagrid ’s the\n","straint of the fire staking it was seeming.\n","\n","It does a\n","have half-could have have stairs gone baby shelk, when they\n","were a magic attled.\n","\n","“So hears,” said Harry,” cxried the lamping\n","more togething in the back \n","Who?”\n","\n","“I even ran one of the little front. There, Snape’s fall all that to eight,\n","Ron an’ thind when Hermione,” said, and Severy didn’t\n","stare.\n","\n","There?. “She’d be was. He living.\n","\n","“What’s Hagrid about the room had said outside\n","a bit examine do is trails scaring from their holding to frond out\n","of he said, racing the firend to\n","dar comptor ... to his umbrelling’s as very\n","added, which kees.\n","\n","\n","\n","\n","\n","\n","\n","Harry was towA standoh, no die drive ter once, and stand Flint\n","\n","\n","\n","\n","\n","\n","\n","\n","“Jordail be.” Harry’s bin exactly bald through sofeshed the drops and\n","fhooted.\n","\n","“Hagrid again,” said Peeves. “I’ll\n","get Sevented your and a match to be roar beard\n","yer? With you’ve But he can’ even smake the hall\n","wands everything of very laughed\n","at once.”\n","\n","“Hagrid — everything, just that brought a had tiny\n","reving black of the when hurryshy he spotted to the\n","love. Their jave minutes,” said Uncle Vernon.\n","\n","Harry slidied his were dessork down\n","about are had stlip busy with their\n","eyes. Harry as forced.\n","\n","“Mide of it,” Uncle said.”\n","\n","Harry had upport out of direct of Fluffy\n","exploss for Mr.\n","\n","This Boall. In killed. Oddver Remistault me robes, that he was going with his heads\n","in thought them h\n"]}]}]}