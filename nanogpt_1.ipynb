{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LokeshDandumahanti/NanoGPT/blob/main/nanogpt_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOTT9r3UqXHb"
      },
      "source": [
        "#1. Intro\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KARDeenzNEsx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8b67886-7757-4239-f235-14a016219196"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-05 13:12:25--  https://gist.githubusercontent.com/flackend/18014f35d32b37c595b138f666b2108f/raw/99494b71652af807e77560b1d83ebbc5ed4c2f32/sorcerers-stone.txt\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 459564 (449K) [text/plain]\n",
            "Saving to: ‘/content/drive/MyDrive/saved_models/sorcerers-stone.txt’\n",
            "\n",
            "/content/drive/MyDr 100%[===================>] 448.79K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-04-05 13:12:25 (11.4 MB/s) - ‘/content/drive/MyDrive/saved_models/sorcerers-stone.txt’ saved [459564/459564]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
        "!wget https://gist.githubusercontent.com/flackend/18014f35d32b37c595b138f666b2108f/raw/99494b71652af807e77560b1d83ebbc5ed4c2f32/sorcerers-stone.txt -O \"/content/drive/MyDrive/saved_models/sorcerers-stone.txt\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U keepsake"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QDct8xY9nQB",
        "outputId": "6b6c5455-fc0e-4571-a3d4-c26b93770700"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keepsake\n",
            "  Downloading keepsake-0.4.2-py3-none-manylinux1_x86_64.whl (16.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: googleapis-common-protos[grpc]>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from keepsake) (1.63.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from googleapis-common-protos[grpc]>=1.0.0->keepsake) (3.20.3)\n",
            "Requirement already satisfied: grpcio<2.0.0.dev0,>=1.44.0 in /usr/local/lib/python3.10/dist-packages (from googleapis-common-protos[grpc]>=1.0.0->keepsake) (1.62.1)\n",
            "Installing collected packages: keepsake\n",
            "Successfully installed keepsake-0.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA9I5yBdPGnT"
      },
      "source": [
        "#2. Reading and exploring the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbXThzYzqhS1"
      },
      "outputs": [],
      "source": [
        "# read it in to inspect it\n",
        "with open('/content/drive/MyDrive/saved_models/sorcerers-stone.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rW5Vh-_009x_",
        "outputId": "f493ac74-7ae2-4220-8a21-c40fadbdb8b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  441832\n"
          ]
        }
      ],
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZduYxLyqNQR",
        "outputId": "41117624-d628-4872-d49e-f733951533d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THE BOY WHO LIVED\n",
            "\n",
            "Mr. and Mrs. Dursley, of number four, Privet Drive,\n",
            "were proud to say that they were perfectly normal,\n",
            "thank you very much. They were the last people you’d\n",
            "expect to be involved in anything strange or\n",
            "mysterious, because they just didn’t hold with such\n",
            "nonsense.\n",
            "\n",
            "Mr. Dursley was the director of a firm called\n",
            "Grunnings, which made drills. He was a big, beefy\n",
            "man with hardly any neck, although he did have a\n",
            "very large mustache. Mrs. Dursley was thin and\n",
            "blonde and had nearly twice the usual amount of\n",
            "neck, which came in very useful as she spent so\n",
            "much of her time craning over garden fences, spying\n",
            "on the neighbors. The Dursley s had a small son\n",
            "called Dudley and in their opinion there was no finer\n",
            "boy anywhere.\n",
            "\n",
            "The Dursleys had everything they wanted, but they\n",
            "also had a secret, and their greatest fear was that\n",
            "somebody would discover it. They didn’t think they\n",
            "could bear it if anyone found out about the Potters.\n",
            "Mrs. Potter was Mrs. Dursley’s sister, but they hadn’t\n"
          ]
        }
      ],
      "source": [
        "#let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paPGTnoZP0SV"
      },
      "source": [
        "#3. Tokenization and train/val split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXTe4niX09fl"
      },
      "source": [
        "1. here basically all the characters are sorted and then assigned a number for each one of them.\n",
        "2. then each of the character is then encodeded to a number till whole dataset is converted into a string of numbers\n",
        "3. then it is broke into training and testing dataset\n",
        "4. After that a neuron is"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7spV-TD0PFI",
        "outputId": "01be0a9a-b45c-40dd-9c78-fb6ca6d0ffbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !\"'(),-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWYZ\\abcdefghijklmnopqrstuvwxyz—‘’“”•■\n",
            "83\n"
          ]
        }
      ],
      "source": [
        "#here are all the unique characters that occur in this text\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9p3K8SN-dKu",
        "outputId": "28bfa6c8-ebbf-44da-a909-ddbcc1731fce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[57, 58, 1, 69, 57, 54, 67, 54]\n",
            "hi there\n"
          ]
        }
      ],
      "source": [
        "#create a mapping from characters to integers\n",
        "\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder : take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "print(encode(\"hi there\"))\n",
        "print(decode(encode(\"hi there\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wp9Ua1mU_V2E",
        "outputId": "89c450aa-630a-4168-8606-c5fde39ec178"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([441832]) torch.int64\n",
            "tensor([43, 31, 28,  1, 25, 38, 47,  1, 46, 31, 38,  1, 35, 32, 45, 28, 27,  0,\n",
            "         0, 36, 67,  9,  1, 50, 63, 53,  1, 36, 67, 68,  9,  1, 27, 70, 67, 68,\n",
            "        61, 54, 74,  7,  1, 64, 55,  1, 63, 70, 62, 51, 54, 67,  1, 55, 64, 70,\n",
            "        67,  7,  1, 39, 67, 58, 71, 54, 69,  1, 27, 67, 58, 71, 54,  7,  0, 72,\n",
            "        54, 67, 54,  1, 65, 67, 64, 70, 53,  1, 69, 64,  1, 68, 50, 74,  1, 69,\n",
            "        57, 50, 69,  1, 69, 57, 54, 74,  1, 72, 54, 67, 54,  1, 65, 54, 67, 55,\n",
            "        54, 52, 69, 61, 74,  1, 63, 64, 67, 62, 50, 61,  7,  0, 69, 57, 50, 63,\n",
            "        60,  1, 74, 64, 70,  1, 71, 54, 67, 74,  1, 62, 70, 52, 57,  9,  1, 43,\n",
            "        57, 54, 74,  1, 72, 54, 67, 54,  1, 69, 57, 54,  1, 61, 50, 68, 69,  1,\n",
            "        65, 54, 64, 65, 61, 54,  1, 74, 64, 70, 78, 53,  0, 54, 73, 65, 54, 52,\n",
            "        69,  1, 69, 64,  1, 51, 54,  1, 58, 63, 71, 64, 61, 71, 54, 53,  1, 58,\n",
            "        63,  1, 50, 63, 74, 69, 57, 58, 63, 56,  1, 68, 69, 67, 50, 63, 56, 54,\n",
            "         1, 64, 67,  0, 62, 74, 68, 69, 54, 67, 58, 64, 70, 68,  7,  1, 51, 54,\n",
            "        52, 50, 70, 68, 54,  1, 69, 57, 54, 74,  1, 59, 70, 68, 69,  1, 53, 58,\n",
            "        53, 63, 78, 69,  1, 57, 64, 61, 53,  1, 72, 58, 69, 57,  1, 68, 70, 52,\n",
            "        57,  0, 63, 64, 63, 68, 54, 63, 68, 54,  9,  0,  0, 36, 67,  9,  1, 27,\n",
            "        70, 67, 68, 61, 54, 74,  1, 72, 50, 68,  1, 69, 57, 54,  1, 53, 58, 67,\n",
            "        54, 52, 69, 64, 67,  1, 64, 55,  1, 50,  1, 55, 58, 67, 62,  1, 52, 50,\n",
            "        61, 61, 54, 53,  0, 30, 67, 70, 63, 63, 58, 63, 56, 68,  7,  1, 72, 57,\n",
            "        58, 52, 57,  1, 62, 50, 53, 54,  1, 53, 67, 58, 61, 61, 68,  9,  1, 31,\n",
            "        54,  1, 72, 50, 68,  1, 50,  1, 51, 58, 56,  7,  1, 51, 54, 54, 55, 74,\n",
            "         0, 62, 50, 63,  1, 72, 58, 69, 57,  1, 57, 50, 67, 53, 61, 74,  1, 50,\n",
            "        63, 74,  1, 63, 54, 52, 60,  7,  1, 50, 61, 69, 57, 64, 70, 56, 57,  1,\n",
            "        57, 54,  1, 53, 58, 53,  1, 57, 50, 71, 54,  1, 50,  0, 71, 54, 67, 74,\n",
            "         1, 61, 50, 67, 56, 54,  1, 62, 70, 68, 69, 50, 52, 57, 54,  9,  1, 36,\n",
            "        67, 68,  9,  1, 27, 70, 67, 68, 61, 54, 74,  1, 72, 50, 68,  1, 69, 57,\n",
            "        58, 63,  1, 50, 63, 53,  0, 51, 61, 64, 63, 53, 54,  1, 50, 63, 53,  1,\n",
            "        57, 50, 53,  1, 63, 54, 50, 67, 61, 74,  1, 69, 72, 58, 52, 54,  1, 69,\n",
            "        57, 54,  1, 70, 68, 70, 50, 61,  1, 50, 62, 64, 70, 63, 69,  1, 64, 55,\n",
            "         0, 63, 54, 52, 60,  7,  1, 72, 57, 58, 52, 57,  1, 52, 50, 62, 54,  1,\n",
            "        58, 63,  1, 71, 54, 67, 74,  1, 70, 68, 54, 55, 70, 61,  1, 50, 68,  1,\n",
            "        68, 57, 54,  1, 68, 65, 54, 63, 69,  1, 68, 64,  0, 62, 70, 52, 57,  1,\n",
            "        64, 55,  1, 57, 54, 67,  1, 69, 58, 62, 54,  1, 52, 67, 50, 63, 58, 63,\n",
            "        56,  1, 64, 71, 54, 67,  1, 56, 50, 67, 53, 54, 63,  1, 55, 54, 63, 52,\n",
            "        54, 68,  7,  1, 68, 65, 74, 58, 63, 56,  0, 64, 63,  1, 69, 57, 54,  1,\n",
            "        63, 54, 58, 56, 57, 51, 64, 67, 68,  9,  1, 43, 57, 54,  1, 27, 70, 67,\n",
            "        68, 61, 54, 74,  1, 68,  1, 57, 50, 53,  1, 50,  1, 68, 62, 50, 61, 61,\n",
            "         1, 68, 64, 63,  0, 52, 50, 61, 61, 54, 53,  1, 27, 70, 53, 61, 54, 74,\n",
            "         1, 50, 63, 53,  1, 58, 63,  1, 69, 57, 54, 58, 67,  1, 64, 65, 58, 63,\n",
            "        58, 64, 63,  1, 69, 57, 54, 67, 54,  1, 72, 50, 68,  1, 63, 64,  1, 55,\n",
            "        58, 63, 54, 67,  0, 51, 64, 74,  1, 50, 63, 74, 72, 57, 54, 67, 54,  9,\n",
            "         0,  0, 43, 57, 54,  1, 27, 70, 67, 68, 61, 54, 74, 68,  1, 57, 50, 53,\n",
            "         1, 54, 71, 54, 67, 74, 69, 57, 58, 63, 56,  1, 69, 57, 54, 74,  1, 72,\n",
            "        50, 63, 69, 54, 53,  7,  1, 51, 70, 69,  1, 69, 57, 54, 74,  0, 50, 61,\n",
            "        68, 64,  1, 57, 50, 53,  1, 50,  1, 68, 54, 52, 67, 54, 69,  7,  1, 50,\n",
            "        63, 53,  1, 69, 57, 54, 58, 67,  1, 56, 67, 54, 50, 69, 54, 68, 69,  1,\n",
            "        55, 54, 50, 67,  1, 72, 50, 68,  1, 69, 57, 50, 69,  0, 68, 64, 62, 54,\n",
            "        51, 64, 53, 74,  1, 72, 64, 70, 61, 53,  1, 53, 58, 68, 52, 64, 71, 54,\n",
            "        67,  1, 58, 69,  9,  1, 43, 57, 54, 74,  1, 53, 58, 53, 63, 78, 69,  1,\n",
            "        69, 57, 58, 63, 60,  1, 69, 57, 54, 74,  0, 52, 64, 70, 61, 53,  1, 51,\n",
            "        54, 50, 67,  1, 58, 69,  1, 58, 55,  1, 50, 63, 74, 64, 63, 54,  1, 55,\n",
            "        64, 70, 63, 53,  1, 64, 70, 69,  1, 50, 51, 64, 70, 69,  1, 69, 57, 54,\n",
            "         1, 39, 64, 69, 69, 54, 67, 68,  9,  0, 36, 67, 68,  9,  1, 39, 64, 69,\n",
            "        69, 54, 67,  1, 72, 50, 68,  1, 36, 67, 68,  9,  1, 27, 70, 67, 68, 61,\n",
            "        54, 74, 78, 68,  1, 68, 58, 68, 69, 54, 67,  7,  1, 51, 70, 69,  1, 69,\n",
            "        57, 54, 74,  1, 57, 50, 53, 63, 78, 69])\n"
          ]
        }
      ],
      "source": [
        "# let's now encode the entire text dataset and store it into a'torch.Tensor\n",
        "import torch # we use PyTorch: http://pytorch.org\n",
        "data = torch.tensor(encode(text), dtype = torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtYfoMMEAeBx"
      },
      "outputs": [],
      "source": [
        "#Let's now split up the data into train and validation sets\n",
        "\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7V320eSA1h1",
        "outputId": "23090285-67ee-48e4-b75a-594b37eb131d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([43, 31, 28,  1, 25, 38, 47,  1, 46])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "block_size = 8\n",
        "train_data[:block_size + 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1de-iNl7A-3A",
        "outputId": "004f83bf-a4fe-4f85-d2e2-e6aaf702d0ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([43]) the target: 31\n",
            "when input is tensor([43, 31]) the target: 28\n",
            "when input is tensor([43, 31, 28]) the target: 1\n",
            "when input is tensor([43, 31, 28,  1]) the target: 25\n",
            "when input is tensor([43, 31, 28,  1, 25]) the target: 38\n",
            "when input is tensor([43, 31, 28,  1, 25, 38]) the target: 47\n",
            "when input is tensor([43, 31, 28,  1, 25, 38, 47]) the target: 1\n",
            "when input is tensor([43, 31, 28,  1, 25, 38, 47,  1]) the target: 46\n"
          ]
        }
      ],
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f\"when input is {context} the target: {target}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGQAegsZQH4e"
      },
      "source": [
        "#4. Data loader : batches of chunks of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4zovBWGBd-N",
        "outputId": "5e39cc78-82e1-4e84-bed0-389dcb6fdcab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[55,  1, 69, 57, 54,  0, 27, 70],\n",
            "        [53,  1, 58, 69,  1, 64, 63, 69],\n",
            "        [ 1, 65, 64, 58, 63, 69, 68,  1],\n",
            "        [ 1, 64, 70, 69,  1, 58, 63, 69]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[ 1, 69, 57, 54,  0, 27, 70, 67],\n",
            "        [ 1, 58, 69,  1, 64, 63, 69, 64],\n",
            "        [65, 64, 58, 63, 69, 68,  1, 55],\n",
            "        [64, 70, 69,  1, 58, 63, 69, 64]])\n",
            "-----\n",
            "when input is [55] the target: 1\n",
            "when input is [55, 1] the target: 69\n",
            "when input is [55, 1, 69] the target: 57\n",
            "when input is [55, 1, 69, 57] the target: 54\n",
            "when input is [55, 1, 69, 57, 54] the target: 0\n",
            "when input is [55, 1, 69, 57, 54, 0] the target: 27\n",
            "when input is [55, 1, 69, 57, 54, 0, 27] the target: 70\n",
            "when input is [55, 1, 69, 57, 54, 0, 27, 70] the target: 67\n",
            "when input is [53] the target: 1\n",
            "when input is [53, 1] the target: 58\n",
            "when input is [53, 1, 58] the target: 69\n",
            "when input is [53, 1, 58, 69] the target: 1\n",
            "when input is [53, 1, 58, 69, 1] the target: 64\n",
            "when input is [53, 1, 58, 69, 1, 64] the target: 63\n",
            "when input is [53, 1, 58, 69, 1, 64, 63] the target: 69\n",
            "when input is [53, 1, 58, 69, 1, 64, 63, 69] the target: 64\n",
            "when input is [1] the target: 65\n",
            "when input is [1, 65] the target: 64\n",
            "when input is [1, 65, 64] the target: 58\n",
            "when input is [1, 65, 64, 58] the target: 63\n",
            "when input is [1, 65, 64, 58, 63] the target: 69\n",
            "when input is [1, 65, 64, 58, 63, 69] the target: 68\n",
            "when input is [1, 65, 64, 58, 63, 69, 68] the target: 1\n",
            "when input is [1, 65, 64, 58, 63, 69, 68, 1] the target: 55\n",
            "when input is [1] the target: 64\n",
            "when input is [1, 64] the target: 70\n",
            "when input is [1, 64, 70] the target: 69\n",
            "when input is [1, 64, 70, 69] the target: 1\n",
            "when input is [1, 64, 70, 69, 1] the target: 58\n",
            "when input is [1, 64, 70, 69, 1, 58] the target: 63\n",
            "when input is [1, 64, 70, 69, 1, 58, 63] the target: 69\n",
            "when input is [1, 64, 70, 69, 1, 58, 63, 69] the target: 64\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4\n",
        "block_size = 8\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i : i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  return x,y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('-----')\n",
        "\n",
        "for b in range(batch_size):\n",
        "  for t in range(block_size):\n",
        "    context = xb[b, :t+1]\n",
        "    target = yb[b,t]\n",
        "    print(f\"when input is {context.tolist()} the target: {target}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_POol24Dx_F",
        "outputId": "abaee46e-1025-4974-c8ad-aa05b2705fc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[55,  1, 69, 57, 54,  0, 27, 70],\n",
            "        [53,  1, 58, 69,  1, 64, 63, 69],\n",
            "        [ 1, 65, 64, 58, 63, 69, 68,  1],\n",
            "        [ 1, 64, 70, 69,  1, 58, 63, 69]])\n"
          ]
        }
      ],
      "source": [
        "print(xb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWJ0Z1qpQllR"
      },
      "source": [
        "#5. Simplest baseline: bigram language model, loss, generation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERBw3OZ8D2sh",
        "outputId": "fbffc6b1-aa34-44b5-bd59-dc25d05af9a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 83])\n",
            "tensor(4.3570, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "44;5’L,OB)!,2W/()7fd Ckxae\"jR!Dtre0qUG“kB5(;QW•N\n",
            "G—oAhhv\"SsGgj4yF-H9q:Ug■jgDJ4S11Bv!6Ulpnlitw;h’()KP\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HrBq6TYfhNO"
      },
      "source": [
        "#6. Training the Bigram Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZQ1l4KnRSex"
      },
      "outputs": [],
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f_7r8f9RchN",
        "outputId": "8bb218c7-1be8-4d4e-8d60-bed7d1c0cc5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.714195728302002\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "for steps in range(100): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Il3MOkxh9Gu"
      },
      "source": [
        "#7. Port our code to a script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mDRWnnmRg3Q",
        "outputId": "3990ab54-e430-40b0-b16a-8b80bd1df7fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ")T\n",
            "UT6t•-!K?n-Jqn3nIrtE\\MY,3nb849Gy'hyNb1!?ZWlElwlwp■Lgnsod‘,J\"4E/o'Z,lPA8cT‘QJ;l4GFll:qL!hztmoT!RJtKPxx:cJ!aCnm-H4Pxbnw?- k!ERFk\"y0n-(!NkWYK7hC2NY■xO75TMB;6,Z'Lz C\n",
            "egMj2pf—sOgrYW9'—M l4•12\"yF6orjlaO.”y'5RSEQ7ZLr6z2 k‘WIkPl0l(lC\n",
            "lW928?7s0a“pntsESg?5■‘n6OFaKao“UL’rLP;;n-oJjOESGsVT,JgsJl-RCh!7IJwZ\n",
            "akSn\n",
            ":xSW-J.uS”Vq)nh:e0■DZ■I’e,P)d\n",
            "w‘nGpmx, WeZW(BR)\"2Yj7vLS)TFy‘”tm N\n",
            "5AbkFj•k.,ZKy3FKH49RRMOYMs Na“p8Nx8NI3”■M?OBzK84V”tY'—ISuqGsfC■1FJHQ5W/Pz.LrKsz):Y5vor5K0nt84s”:x‘/gN3ntBR\"1qT2NwZnx!vKZE3N.w6r?f?T \n"
          ]
        }
      ],
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPb9rcNVRktD"
      },
      "source": [
        "#8. The trick in self-attention: matrix multiply as weight aggregation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcwiGHGcSLoI",
        "outputId": "c51e975e-4582-482a-fae9-a577bf3c0f57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ],
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHRcjq1NiT_M"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkPhcJsvSRff",
        "outputId": "87518294-45d1-47bd-9424-cc7de67887c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# consider the following toy example:\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-SCPrvMSV87"
      },
      "outputs": [],
      "source": [
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc7x_2ZOSwds"
      },
      "source": [
        "#9. V2: Using matrix multiplication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Evfz-ziSpCY",
        "outputId": "e0ff54b9-b2e8-4791-de54-4a87af77ab79"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# version 2: using matrix multiply for a weighted aggregation\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "torch.allclose(xbow, xbow2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCeAMYnKS7iF"
      },
      "source": [
        "#10. V3. Adding softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADZyCNR7THtR",
        "outputId": "e414e79c-6009-482f-82cf-78a6746c01e6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Wgr06ZLTNvP"
      },
      "source": [
        "#12. V4. self attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzUOVIQ8Te8K",
        "outputId": "619ea5f6-081e-4f0b-da68-42275cb1f1ba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4LznjneTjur",
        "outputId": "315e58d5-8e11-4b73-b901-1045124a3faf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "wei[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evGWJ5IQTNfh"
      },
      "source": [
        "#13. n1: attention as communication\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8vhwrkKT1OL"
      },
      "source": [
        "Notes:\n",
        "\n",
        "1. Attention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "2. There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
        "3. Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "4.In an \"encoder\" attention block just delete the single line that does masking with tril, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "5. \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "6. \"Scaled\" attention additional divides wei by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kFVjkoXUWct"
      },
      "source": [
        "#14. Inserting a single self-attention block to our network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjkLK0DtTxvj"
      },
      "outputs": [],
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiiEcX2_UrTd",
        "outputId": "a94a41c8-bb48-462c-d38d-f2dce2f0866a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0449)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "k.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sc-hkTO4VIzS",
        "outputId": "09b61165-914d-4222-b11d-10069010f0d1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0700)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "q.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68iTSVfuVP2Z",
        "outputId": "76a1f573-b0de-48a8-a1e0-5e1341a8f1d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0918)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "wei.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDSch3WwVSdU",
        "outputId": "36a00e07-ae21-437a-f587-29f37c8ce6ff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wnWmXokVh_-",
        "outputId": "8cd353d6-ed44-4b83-aa88-ef50b1c77ed9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1BVAqFHVkof",
        "outputId": "5a4cf60e-725e-4416-ce77-8f6d36fede6e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMRL8gvEVpxY",
        "outputId": "689fd43f-9091-4ee4-fe2f-ec4feda2dbbf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ML5-YitWVtrZ"
      },
      "outputs": [],
      "source": [
        "# French to English translation example:\n",
        "\n",
        "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
        "# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKIlukc1WIu8"
      },
      "source": [
        "#15. Creating Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsNz-qlW5EsN"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPCwf6jqWRMS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 6000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 6\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('/content/drive/MyDrive/saved_models/sorcerers-stone.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUFNWtkaw8tl",
        "outputId": "7fd227a4-3bd6-43cb-98fc-8d74cd763f86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.311635 M parameters\n"
          ]
        }
      ],
      "source": [
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "W35WXSLdbasw",
        "outputId": "8686f6cd-348b-4c7f-9a7d-3c23ec11e5f6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nmodel = BigramLanguageModel()\\nm = model.to(device)\\n# print the number of parameters in the model\\nprint(sum(p.numel() for p in m.parameters())/1e6, \\'M parameters\\')\\n\\n# create a PyTorch optimizer\\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\\n\\nfor iter in range(max_iters):\\n\\n    # every once in a while evaluate the loss on train and val sets\\n    if iter % eval_interval == 0 or iter == max_iters - 1:\\n        losses = estimate_loss()\\n        print(f\"step {iter}: train loss {losses[\\'train\\']:.4f}, val loss {losses[\\'val\\']:.4f}\")\\n\\n    # sample a batch of data\\n    xb, yb = get_batch(\\'train\\')\\n\\n    # evaluate the loss\\n    logits, loss = model(xb, yb)\\n    optimizer.zero_grad(set_to_none=True)\\n    loss.backward()\\n    optimizer.step()'"
            ]
          },
          "execution_count": 140,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bfe2O7Wiw1zB",
        "outputId": "d73a7251-d87f-4f16-ad73-a35637936b08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.5200, val loss 4.5310\n",
            "step 100: train loss 2.6541, val loss 2.6362\n",
            "step 200: train loss 2.5006, val loss 2.4894\n",
            "step 300: train loss 2.4195, val loss 2.4080\n",
            "step 400: train loss 2.3398, val loss 2.3475\n",
            "step 500: train loss 2.2717, val loss 2.2605\n",
            "step 600: train loss 2.1980, val loss 2.2078\n",
            "step 700: train loss 2.1333, val loss 2.1312\n",
            "step 800: train loss 2.0697, val loss 2.0723\n",
            "step 900: train loss 2.0147, val loss 2.0210\n",
            "step 1000: train loss 1.9837, val loss 1.9836\n",
            "step 1100: train loss 1.9343, val loss 1.9436\n",
            "step 1200: train loss 1.9028, val loss 1.9152\n",
            "step 1300: train loss 1.8835, val loss 1.9049\n",
            "step 1400: train loss 1.8561, val loss 1.8663\n",
            "step 1500: train loss 1.8300, val loss 1.8534\n",
            "step 1600: train loss 1.8075, val loss 1.8245\n",
            "step 1700: train loss 1.7908, val loss 1.7992\n",
            "step 1800: train loss 1.7597, val loss 1.7942\n",
            "step 1900: train loss 1.7589, val loss 1.7973\n",
            "step 2000: train loss 1.7392, val loss 1.7780\n",
            "step 2100: train loss 1.7206, val loss 1.7543\n",
            "step 2200: train loss 1.7146, val loss 1.7476\n",
            "step 2300: train loss 1.7101, val loss 1.7373\n",
            "step 2400: train loss 1.6837, val loss 1.7062\n",
            "step 2500: train loss 1.6671, val loss 1.7130\n",
            "step 2600: train loss 1.6575, val loss 1.6991\n",
            "step 2700: train loss 1.6566, val loss 1.6963\n",
            "step 2800: train loss 1.6419, val loss 1.6708\n",
            "step 2900: train loss 1.6304, val loss 1.6728\n",
            "step 3000: train loss 1.6292, val loss 1.6663\n",
            "step 3100: train loss 1.6176, val loss 1.6613\n",
            "step 3200: train loss 1.6079, val loss 1.6498\n",
            "step 3300: train loss 1.6143, val loss 1.6582\n",
            "step 3400: train loss 1.6033, val loss 1.6449\n",
            "step 3500: train loss 1.5931, val loss 1.6267\n",
            "step 3600: train loss 1.5805, val loss 1.6248\n",
            "step 3700: train loss 1.5818, val loss 1.6318\n",
            "step 3800: train loss 1.5652, val loss 1.6185\n",
            "step 3900: train loss 1.5705, val loss 1.6191\n",
            "step 4000: train loss 1.5709, val loss 1.6259\n",
            "step 4100: train loss 1.5605, val loss 1.6354\n",
            "step 4200: train loss 1.5515, val loss 1.6132\n",
            "step 4300: train loss 1.5503, val loss 1.6085\n",
            "step 4400: train loss 1.5376, val loss 1.5878\n",
            "step 4500: train loss 1.5370, val loss 1.6047\n",
            "step 4600: train loss 1.5351, val loss 1.5832\n",
            "step 4700: train loss 1.5374, val loss 1.6074\n",
            "step 4800: train loss 1.5318, val loss 1.5819\n",
            "step 4900: train loss 1.5200, val loss 1.5775\n",
            "step 5000: train loss 1.5221, val loss 1.5773\n",
            "step 5100: train loss 1.5131, val loss 1.5814\n",
            "step 5200: train loss 1.5216, val loss 1.5754\n",
            "step 5300: train loss 1.5043, val loss 1.5725\n",
            "step 5400: train loss 1.5119, val loss 1.5756\n",
            "step 5500: train loss 1.4911, val loss 1.5695\n",
            "step 5600: train loss 1.4967, val loss 1.5579\n",
            "step 5700: train loss 1.4938, val loss 1.5496\n",
            "step 5800: train loss 1.4825, val loss 1.5562\n",
            "step 5900: train loss 1.4793, val loss 1.5511\n",
            "step 5999: train loss 1.4804, val loss 1.5641\n"
          ]
        }
      ],
      "source": [
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr0327NQqj_l"
      },
      "source": [
        "#16. Saving model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3H0DtRmq86k"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(),'/content/drive/MyDrive/saved_models/modelboy_l6.h5')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NuzjhaOaSUC"
      },
      "source": [
        "#17. Prompting and output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7VjKMqLrWWC"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJryUhL54Os9",
        "outputId": "5c0e661f-0cda-4e8d-b81e-c65efab10cf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "what is the difference between harry and hagrid twin over and broth\n",
            "drofe. Throwing, thought of the troathed\n",
            "on the growies pier and heads, shared vitia a sup) ”\n",
            "gothe. Flaming!”\n",
            "\n",
            "“I’ll goin od lood jon, I tried eer Isna had varching and riblin could be.\n",
            "he threw and squaaatly from pointing himself than rest\n",
            "broom. Throw, but Evera Thous.\n",
            "\n",
            "“Well read onto easten bice kack\n",
            "and brhing the corridors.\n",
            "\n",
            "“I’m seed his going to first,” buars appoathed.\n",
            "\n",
            "Anyther of threw, and Dout it? You not,”\n",
            "beasle Harry tragain.”\n",
            "\n",
            "“No, Peoll’s ofect of ‘necurious a\n",
            "bump it ontention vooiced had an enough a plate hoot\n",
            "class light back. “And was roop coldwing\n",
            "broomsticks why hard gold, “ wream to an oving a such Gringer. Now were whole started narrow chocless, at\n",
            "those innoeth!” he said?”\n",
            "\n",
            "But he or starry, seaving, looking with seen. Wwasn’t\n",
            "do you want to be about a laugh\n",
            "boy ploce hard out up and under down\n",
            "Hagrid that bosning?” Harry rosed piled corridor. “Professor Malkon, doces sharp stlyether later to de.\n",
            "\n",
            "“Thousand hool.” Snape athat Hermione trought tried to the\n",
            "shots, wasn’t back, I tin Malte in they’d hoter has though .. .. \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "“I’ll even line. I met out. Longboct .. I\n",
            "warth.\n",
            "\n",
            "“Not we to do of a day offaint about the wrong togrived away! Che Phothin, though three,\n",
            "\n",
            "“Liook atch, onother was a botic, it’s an atter,\n",
            "and tarter Leet; a’ll dill antick both wollow. Who you seven hold go — that\n",
            "seach Harry’s drosned try teached out of happen, about yously at\n",
            "with Dumbledore’s owled to do keep and what’s made\n",
            "carefirasts. They atiman a talk and bungling horrold our book, that\n",
            "the Snotatie. “Wet not Evermy Twowcard, Hermbustardly.”med Ron and a\n",
            "home Like ter, ithinger had mutth his\n",
            "wabe storted in thair and classly toos\n",
            "dickly door.\n",
            "\n",
            "“Quid’s usual exactly bacos of trying\n",
            "eages bave was so pun.\n",
            "\n",
            "“Got,” said Snape bound to stooth, who all tran. I say and teashe, wellent Harry’s watched turned to the en grow, pointing\n",
            "and apaoned upbed althone. “Wrood\n",
            "up or the wand. Hogwarts — hocnap a\n",
            "secondly, but though the swungy talking. They ha\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load the model and move it to the same device as the tokens\n",
        "model = BigramLanguageModel().to(device)\n",
        "state_dict = torch.load('/content/drive/MyDrive/saved_models/modelboy_l6.h5')\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()\n",
        "\n",
        "# Tokenize the starting prompt\n",
        "prompt = \"what is the difference between harry and hagrid\"\n",
        "tokens = torch.tensor(encode(prompt), dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "# Generate the conversation\n",
        "max_tokens = 2000\n",
        "word_count = 0\n",
        "\n",
        "# Generate up to max_tokens words\n",
        "for _ in range(max_tokens):\n",
        "    # Generate the next token\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(tokens, max_new_tokens=1)\n",
        "    new_token = output[:, -1].item()\n",
        "\n",
        "    # Convert the new token to a character and append to the prompt\n",
        "    new_char = itos[new_token]\n",
        "    if new_char == '':\n",
        "      word_count += 1\n",
        "    tokens = torch.cat((tokens, torch.tensor([[new_token]], device=device)), dim=1)\n",
        "\n",
        "    if word_count >= 2000:\n",
        "      break\n",
        "\n",
        "# Convert the token indices to characters and join them into a string\n",
        "generated_text = ''.join([itos[idx] for idx in tokens.squeeze().tolist()])\n",
        "\n",
        "# Print the generated text\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iahifShBUMj7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "ZBR5CFAuVnP_",
        "outputId": "36480ce6-b725-4716-e405-dc5fddaca41c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# List of 50 prompts\\nprompts = [\\n    \"What is the difference between Harry and Hagrid?\",\\n    \"How did Voldemort lose his powers?\",\\n    \"Why did Dumbledore trust Snape?\",\\n    \"Describe a typical day in the life of a Hogwarts student.\",\\n    \"What are some magical creatures Harry encountered during his time at Hogwarts?\",\\n    \"Explain how the Triwizard Tournament works.\",\\n    \"Describe a spell that Harry learned in Defense Against the Dark Arts class.\",\\n    \"What is the significance of Harry\\'s lightning-shaped scar?\",\\n    \"Discuss the relationship between Harry, Ron, and Hermione.\",\\n    \"Describe a Quidditch match at Hogwarts.\",\\n    \"What challenges did Harry face in the Chamber of Secrets?\",\\n    \"Explain the concept of Horcruxes and how they were used by Voldemort.\",\\n    \"Describe a lesson with Professor Snape in Potions class.\",\\n    \"Discuss the role of Dumbledore as headmaster of Hogwarts.\",\\n    \"Explain the history and significance of the Deathly Hallows.\",\\n    \"Describe a feast in the Great Hall at Hogwarts.\",\\n    \"Discuss the rivalry between Gryffindor and Slytherin houses.\",\\n    \"Describe a magical object that plays a key role in the Harry Potter series.\",\\n    \"Explain the process of brewing Polyjuice Potion.\",\\n    \"Discuss the significance of Harry\\'s Patronus.\",\\n    \"Describe a visit to Hogsmeade village.\",\\n    \"Explain the concept of Animagi and how they are registered.\",\\n    \"Discuss the role of house-elves in the Wizarding World.\",\\n    \"Describe a visit to the Forbidden Forest at Hogwarts.\",\\n    \"Explain how the Marauder\\'s Map works.\",\\n    \"Discuss the history and significance of the Mirror of Erised.\",\\n    \"Describe a magical creature from the Harry Potter series and its characteristics.\",\\n    \"Explain the rules and regulations of the Hogwarts Express train.\",\\n    \"Discuss the importance of wandlore in the Wizarding World.\",\\n    \"Describe a magical plant found in the Hogwarts greenhouse.\",\\n    \"Explain the significance of the Sorting Hat ceremony at Hogwarts.\",\\n    \"Discuss the role of the Ministry of Magic in regulating the Wizarding World.\",\\n    \"Describe a magical duel between two characters in the Harry Potter series.\",\\n    \"Explain the role of the Triwizard Cup in the Triwizard Tournament.\",\\n    \"Discuss the significance of the Yule Ball in the fourth Harry Potter book.\",\\n    \"Describe a magical object from the Harry Potter series that has the power to transport individuals.\",\\n    \"Explain the process of creating a Horcrux and why it is considered dark magic.\",\\n    \"Discuss the importance of Quidditch in the Wizarding World.\",\\n    \"Describe a magical creature that is considered dangerous in the Harry Potter series.\",\\n    \"Explain the concept of Occlumency and why it is important for wizards to learn.\",\\n    \"Discuss the role of the Hogwarts Express train in transporting students to Hogwarts.\",\\n    \"Describe a magical artifact that plays a key role in one of the Harry Potter books.\",\\n    \"Explain the concept of the Triwizard Tournament and how it is organized.\",\\n    \"Discuss the importance of the Room of Requirement in the Harry Potter series.\",\\n    \"Describe a magical spell that is used for defensive purposes in the Wizarding World.\",\\n    \"Explain the significance of the prophecy regarding Harry and Voldemort.\",\\n    \"Discuss the role of house points in determining the House Cup winner at Hogwarts.\",\\n    \"Describe a magical creature that is considered to be mythical in the Harry Potter series.\",\\n    \"Explain the concept of the Unbreakable Vow and its consequences.\",\\n    \"Discuss the role of the Hogwarts ghosts in the Harry Potter series.\",\\n    \"Describe a magical object that has the power to manipulate time.\",\\n    \"Explain the significance of Harry\\'s Invisibility Cloak in the Harry Potter series.\"\\n]\\n\\n# Move the entire model to the same device as the tokens\\nmodel.to(device)\\n\\n# Generate text for each prompt\\nfor i, prompt in enumerate(prompts):\\n    print(f\"Generating text for prompt {i + 1}\")\\n\\n    # Tokenize the starting prompt\\n    tokens = torch.tensor(encode(prompt), dtype=torch.long).unsqueeze(0).to(device)\\n    max_tokens = 2000\\n    word_count = 0\\n\\n    # Generate up to max_tokens words\\n    for _ in range(max_tokens):\\n        with torch.no_grad():\\n            output = model.generate(tokens, max_new_tokens=1)\\n        new_token = output[:, -1].item()\\n        new_char = itos[new_token]\\n        if new_char == \\'\\':\\n            word_count += 1\\n        new_tensor = torch.tensor([[new_token]], device=device)\\n        tokens = torch.cat((tokens, new_tensor), dim=1)\\n\\n        if word_count >= 15000:\\n            break\\n\\n    # Convert the token indices to characters and join them into a string\\n    generated_text = \\'\\'.join([itos[idx] for idx in tokens.squeeze().tolist()])\\n    print(generated_text)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "'''\n",
        "# List of 50 prompts\n",
        "prompts = [\n",
        "    \"What is the difference between Harry and Hagrid?\",\n",
        "    \"How did Voldemort lose his powers?\",\n",
        "    \"Why did Dumbledore trust Snape?\",\n",
        "    \"Describe a typical day in the life of a Hogwarts student.\",\n",
        "    \"What are some magical creatures Harry encountered during his time at Hogwarts?\",\n",
        "    \"Explain how the Triwizard Tournament works.\",\n",
        "    \"Describe a spell that Harry learned in Defense Against the Dark Arts class.\",\n",
        "    \"What is the significance of Harry's lightning-shaped scar?\",\n",
        "    \"Discuss the relationship between Harry, Ron, and Hermione.\",\n",
        "    \"Describe a Quidditch match at Hogwarts.\",\n",
        "    \"What challenges did Harry face in the Chamber of Secrets?\",\n",
        "    \"Explain the concept of Horcruxes and how they were used by Voldemort.\",\n",
        "    \"Describe a lesson with Professor Snape in Potions class.\",\n",
        "    \"Discuss the role of Dumbledore as headmaster of Hogwarts.\",\n",
        "    \"Explain the history and significance of the Deathly Hallows.\",\n",
        "    \"Describe a feast in the Great Hall at Hogwarts.\",\n",
        "    \"Discuss the rivalry between Gryffindor and Slytherin houses.\",\n",
        "    \"Describe a magical object that plays a key role in the Harry Potter series.\",\n",
        "    \"Explain the process of brewing Polyjuice Potion.\",\n",
        "    \"Discuss the significance of Harry's Patronus.\",\n",
        "    \"Describe a visit to Hogsmeade village.\",\n",
        "    \"Explain the concept of Animagi and how they are registered.\",\n",
        "    \"Discuss the role of house-elves in the Wizarding World.\",\n",
        "    \"Describe a visit to the Forbidden Forest at Hogwarts.\",\n",
        "    \"Explain how the Marauder's Map works.\",\n",
        "    \"Discuss the history and significance of the Mirror of Erised.\",\n",
        "    \"Describe a magical creature from the Harry Potter series and its characteristics.\",\n",
        "    \"Explain the rules and regulations of the Hogwarts Express train.\",\n",
        "    \"Discuss the importance of wandlore in the Wizarding World.\",\n",
        "    \"Describe a magical plant found in the Hogwarts greenhouse.\",\n",
        "    \"Explain the significance of the Sorting Hat ceremony at Hogwarts.\",\n",
        "    \"Discuss the role of the Ministry of Magic in regulating the Wizarding World.\",\n",
        "    \"Describe a magical duel between two characters in the Harry Potter series.\",\n",
        "    \"Explain the role of the Triwizard Cup in the Triwizard Tournament.\",\n",
        "    \"Discuss the significance of the Yule Ball in the fourth Harry Potter book.\",\n",
        "    \"Describe a magical object from the Harry Potter series that has the power to transport individuals.\",\n",
        "    \"Explain the process of creating a Horcrux and why it is considered dark magic.\",\n",
        "    \"Discuss the importance of Quidditch in the Wizarding World.\",\n",
        "    \"Describe a magical creature that is considered dangerous in the Harry Potter series.\",\n",
        "    \"Explain the concept of Occlumency and why it is important for wizards to learn.\",\n",
        "    \"Discuss the role of the Hogwarts Express train in transporting students to Hogwarts.\",\n",
        "    \"Describe a magical artifact that plays a key role in one of the Harry Potter books.\",\n",
        "    \"Explain the concept of the Triwizard Tournament and how it is organized.\",\n",
        "    \"Discuss the importance of the Room of Requirement in the Harry Potter series.\",\n",
        "    \"Describe a magical spell that is used for defensive purposes in the Wizarding World.\",\n",
        "    \"Explain the significance of the prophecy regarding Harry and Voldemort.\",\n",
        "    \"Discuss the role of house points in determining the House Cup winner at Hogwarts.\",\n",
        "    \"Describe a magical creature that is considered to be mythical in the Harry Potter series.\",\n",
        "    \"Explain the concept of the Unbreakable Vow and its consequences.\",\n",
        "    \"Discuss the role of the Hogwarts ghosts in the Harry Potter series.\",\n",
        "    \"Describe a magical object that has the power to manipulate time.\",\n",
        "    \"Explain the significance of Harry's Invisibility Cloak in the Harry Potter series.\"\n",
        "]\n",
        "\n",
        "# Move the entire model to the same device as the tokens\n",
        "model.to(device)\n",
        "\n",
        "# Generate text for each prompt\n",
        "for i, prompt in enumerate(prompts):\n",
        "    print(f\"Generating text for prompt {i + 1}\")\n",
        "\n",
        "    # Tokenize the starting prompt\n",
        "    tokens = torch.tensor(encode(prompt), dtype=torch.long).unsqueeze(0).to(device)\n",
        "    max_tokens = 2000\n",
        "    word_count = 0\n",
        "\n",
        "    # Generate up to max_tokens words\n",
        "    for _ in range(max_tokens):\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(tokens, max_new_tokens=1)\n",
        "        new_token = output[:, -1].item()\n",
        "        new_char = itos[new_token]\n",
        "        if new_char == '':\n",
        "            word_count += 1\n",
        "        new_tensor = torch.tensor([[new_token]], device=device)\n",
        "        tokens = torch.cat((tokens, new_tensor), dim=1)\n",
        "\n",
        "        if word_count >= 15000:\n",
        "            break\n",
        "\n",
        "    # Convert the token indices to characters and join them into a string\n",
        "    generated_text = ''.join([itos[idx] for idx in tokens.squeeze().tolist()])\n",
        "    print(generated_text)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "nulzoqIDZed2",
        "outputId": "6ffeb8f7-1136-4ac3-d781-c14a05244ef7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# List of 50 prompts\\nprompts = [\\n    \"What is the difference between Harry and Hagrid?\",\\n    \"How did Voldemort lose his powers?\",\\n    \"Why did Dumbledore trust Snape?\",\\n    # Add your remaining prompts here\\n]\\n\\n# Move the entire model to the same device as the tokens\\nmodel.to(device)\\n\\n# Generate the conversation for each prompt\\nfor i, prompt in enumerate(prompts):\\n    print(f\"Generating text for prompt {i + 1}\")\\n    tokens = torch.tensor(encode(prompt), dtype=torch.long).unsqueeze(0).to(device)\\n    max_tokens = 2000\\n    word_count = 0\\n    while True:\\n        with torch.no_grad():\\n            output = model.generate(tokens, max_new_tokens=1)\\n        new_token = output[:, -1].item()\\n        new_char = itos[new_token]\\n        if new_char == \\'\\':\\n            word_count += 1\\n        new_tensor = torch.tensor([[new_token]], device=device)\\n        tokens = torch.cat((tokens, new_tensor), dim=1)\\n\\n        if word_count >= 2000:\\n            break\\n\\n    generated_text = \\'\\'.join([itos[idx] for idx in tokens.squeeze().tolist()])\\n    print(generated_text)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "'''\n",
        "# List of 50 prompts\n",
        "prompts = [\n",
        "    \"What is the difference between Harry and Hagrid?\",\n",
        "    \"How did Voldemort lose his powers?\",\n",
        "    \"Why did Dumbledore trust Snape?\",\n",
        "    # Add your remaining prompts here\n",
        "]\n",
        "\n",
        "# Move the entire model to the same device as the tokens\n",
        "model.to(device)\n",
        "\n",
        "# Generate the conversation for each prompt\n",
        "for i, prompt in enumerate(prompts):\n",
        "    print(f\"Generating text for prompt {i + 1}\")\n",
        "    tokens = torch.tensor(encode(prompt), dtype=torch.long).unsqueeze(0).to(device)\n",
        "    max_tokens = 2000\n",
        "    word_count = 0\n",
        "    while True:\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(tokens, max_new_tokens=1)\n",
        "        new_token = output[:, -1].item()\n",
        "        new_char = itos[new_token]\n",
        "        if new_char == '':\n",
        "            word_count += 1\n",
        "        new_tensor = torch.tensor([[new_token]], device=device)\n",
        "        tokens = torch.cat((tokens, new_tensor), dim=1)\n",
        "\n",
        "        if word_count >= 2000:\n",
        "            break\n",
        "\n",
        "    generated_text = ''.join([itos[idx] for idx in tokens.squeeze().tolist()])\n",
        "    print(generated_text)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1C7VquWlbZs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c96f5e27-4896-487b-b042-0e4937781cda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryCd8P26lmHL",
        "outputId": "ddd8c777-db7d-4155-8044-5308efea9a61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text saved to file.\n"
          ]
        }
      ],
      "source": [
        "# Path to save the file\n",
        "file_path = '/content/drive/My Drive/saved_models/generated_text_l06.txt'\n",
        "\n",
        "# Write text to file\n",
        "with open(file_path, 'w') as file:\n",
        "    file.write(generated_text)\n",
        "\n",
        "print(\"Text saved to file.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqxoKXLJwx2G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tK9RtdG0yMvc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjN3JTdnyQJN"
      },
      "source": [
        "#17. Analysing Accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6WL0cK4yU8o",
        "outputId": "df57156c-816d-4c51-9725-13dc322b82b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of matching words: 144\n",
            "Accuracy: 0.5647058823529412\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import json\n",
        "\n",
        "# Function to tokenize text into words\n",
        "def tokenize(text):\n",
        "    return set(re.findall(r'\\b\\w+\\b', text.lower()))\n",
        "\n",
        "# Load the contents of both files\n",
        "file1_path = '/content/drive/MyDrive/saved_models/sorcerers-stone.txt'\n",
        "file2_path = '/content/drive/MyDrive/saved_models/generated_text_l06.txt'\n",
        "\n",
        "with open(file1_path, 'r', encoding='utf-8') as file1:\n",
        "    text1 = file1.read()\n",
        "\n",
        "with open(file2_path, 'r', encoding='utf-8') as file2:\n",
        "    text2 = file2.read()\n",
        "\n",
        "# Tokenize the text into words\n",
        "words_file1 = tokenize(text1)\n",
        "words_file2 = tokenize(text2)\n",
        "\n",
        "# Calculate the number of words in file 2 that are also in file 1\n",
        "matching_words = len(words_file2.intersection(words_file1))\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = matching_words / len(words_file2) if len(words_file2) > 0 else 0\n",
        "\n",
        "print(f\"Number of matching words: {matching_words}\")\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Save the dictionaries to files\n",
        "dict1_path = '/content/drive/MyDrive/saved_models/dict3.json'\n",
        "dict2_path = '/content/drive/MyDrive/saved_models/dict4.json'\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFuOz8GE-AAO"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "nOTT9r3UqXHb",
        "dA9I5yBdPGnT",
        "paPGTnoZP0SV",
        "CGQAegsZQH4e",
        "EWJ0Z1qpQllR",
        "2HrBq6TYfhNO",
        "9Il3MOkxh9Gu",
        "tPb9rcNVRktD",
        "zc7x_2ZOSwds",
        "HCeAMYnKS7iF",
        "7Wgr06ZLTNvP",
        "evGWJ5IQTNfh",
        "5kFVjkoXUWct"
      ],
      "provenance": [],
      "mount_file_id": "1-8d6hucTibNQrLvxmQFiaRZ8ek9N9k4n",
      "authorship_tag": "ABX9TyOatj5+kNoLEjan3OS2xt8w",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}